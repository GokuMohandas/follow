{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Documentation Operations : Overview of common operational workflows. Configuration : Configuration parameters used for the system. TagIfAI reference : Detailed documentation of each script and it's respective functions and classes. MLOps Lessons Learn how to apply ML to build a production grade product to deliver value. Lessons: https://madewithml.com/ Code: GokuMohandas/MLOps","title":"TagIfAI"},{"location":"#documentation","text":"Operations : Overview of common operational workflows. Configuration : Configuration parameters used for the system. TagIfAI reference : Detailed documentation of each script and it's respective functions and classes.","title":"Documentation"},{"location":"#mlops-lessons","text":"Learn how to apply ML to build a production grade product to deliver value. Lessons: https://madewithml.com/ Code: GokuMohandas/MLOps","title":"MLOps Lessons"},{"location":"app/api/","text":"construct_response ( f ) Construct a JSON response for an endpoint's results. Source code in app/api.py def construct_response ( f ): \"\"\"Construct a JSON response for an endpoint's results.\"\"\" @wraps ( f ) def wrap ( request : Request , * args , ** kwargs ): results = f ( request , * args , ** kwargs ) # Construct response response = { \"message\" : results [ \"message\" ], \"method\" : request . method , \"status-code\" : results [ \"status-code\" ], \"timestamp\" : datetime . now () . isoformat (), \"url\" : request . url . _url , } # Add data if \"data\" in results : response [ \"data\" ] = results [ \"data\" ] return response return wrap","title":"Api"},{"location":"app/api/#app.api.construct_response","text":"Construct a JSON response for an endpoint's results. Source code in app/api.py def construct_response ( f ): \"\"\"Construct a JSON response for an endpoint's results.\"\"\" @wraps ( f ) def wrap ( request : Request , * args , ** kwargs ): results = f ( request , * args , ** kwargs ) # Construct response response = { \"message\" : results [ \"message\" ], \"method\" : request . method , \"status-code\" : results [ \"status-code\" ], \"timestamp\" : datetime . now () . isoformat (), \"url\" : request . url . _url , } # Add data if \"data\" in results : response [ \"data\" ] = results [ \"data\" ] return response return wrap","title":"construct_response()"},{"location":"app/schemas/","text":"","title":"Schemas"},{"location":"config/config/","text":"In this file we're setting up the configuration needed for all our workflows. First up is creating required directories so we can save and load from them: # Directories BASE_DIR = Path ( __file__ ) . parent . parent . absolute () CONFIG_DIR = Path ( BASE_DIR , \"config\" ) LOGS_DIR = Path ( BASE_DIR , \"logs\" ) DATA_DIR = Path ( BASE_DIR , \"data\" ) MODEL_REGISTRY = Path ( BASE_DIR , \"experiments\" ) DVC_REMOTE_STORAGE = Path ( BASE_DIR , \"tmp/dvcstore\" ) # Create dirs LOGS_DIR . mkdir ( parents = True , exist_ok = True ) DATA_DIR . mkdir ( parents = True , exist_ok = True ) MODEL_REGISTRY . mkdir ( parents = True , exist_ok = True ) DVC_REMOTE_STORAGE . mkdir ( parents = True , exist_ok = True ) Then, we'll set the tracking URI for all MLFlow experiments: # MLFlow mlflow . set_tracking_uri ( \"file://\" + str ( MODEL_REGISTRY . absolute ())) Finally, we'll establish our logger using the logging_config dictionary: # Logger logging . config . dictConfig ( logging_config ) logger = logging . getLogger ( \"root\" ) logger . handlers [ 0 ] = RichHandler ( markup = True )","title":"Configurations"},{"location":"tagifai/data/","text":"CNNTextDataset Create torch.utils.data.Dataset objects to use for efficiently feeding data into our models. Usage: # Create dataset X , y = data dataset = CNNTextDataset ( X = X , y = y , max_filter_size = max_filter_size ) # Create dataloaders dataloader = dataset . create_dataloader ( batch_size = batch_size ) collate_fn ( self , batch ) Processing on a batch. It's used to override the default collate_fn in torch.utils.data.DataLoader . Parameters: Name Type Description Default batch List List of inputs and outputs. required Returns: Type Description Tuple Processed inputs and outputs. Source code in tagifai/data.py def collate_fn ( self , batch : List ) -> Tuple : \"\"\"Processing on a batch. It's used to override the default `collate_fn` in `torch.utils.data.DataLoader`. Args: batch (List): List of inputs and outputs. Returns: Processed inputs and outputs. \"\"\" # Get inputs batch = np . array ( batch , dtype = object ) X = batch [:, 0 ] y = np . stack ( batch [:, 1 ], axis = 0 ) # Pad inputs X = pad_sequences ( sequences = X , max_seq_len = self . max_filter_size ) # Cast X = torch . LongTensor ( X . astype ( np . int32 )) y = torch . FloatTensor ( y . astype ( np . int32 )) return X , y create_dataloader ( self , batch_size , shuffle = False , drop_last = False ) Create dataloaders to load batches with. Usage: # Create dataset X , y = data dataset = CNNTextDataset ( X = X , y = y , max_filter_size = max_filter_size ) # Create dataloaders dataloader = dataset . create_dataloader ( batch_size = batch_size ) Parameters: Name Type Description Default batch_size int Number of samples per batch. required shuffle bool Shuffle each batch. Defaults to False. False drop_last bool Drop the last batch if it's less than batch_size . Defaults to False. False Returns: Type Description DataLoader Torch dataloader to load batches with. Source code in tagifai/data.py def create_dataloader ( self , batch_size : int , shuffle : bool = False , drop_last : bool = False ) -> torch . utils . data . DataLoader : \"\"\"Create dataloaders to load batches with. Usage: ```python # Create dataset X, y = data dataset = CNNTextDataset(X=X, y=y, max_filter_size=max_filter_size) # Create dataloaders dataloader = dataset.create_dataloader(batch_size=batch_size) ``` Args: batch_size (int): Number of samples per batch. shuffle (bool, optional): Shuffle each batch. Defaults to False. drop_last (bool, optional): Drop the last batch if it's less than `batch_size`. Defaults to False. Returns: Torch dataloader to load batches with. \"\"\" return torch . utils . data . DataLoader ( dataset = self , batch_size = batch_size , collate_fn = self . collate_fn , shuffle = shuffle , drop_last = drop_last , pin_memory = True , ) LabelEncoder Encode labels into unique indices. Usage: # Encode labels label_encoder = LabelEncoder () label_encoder . fit ( labels ) y = label_encoder . encode ( labels ) MultiClassLabelEncoder Encode labels into unique indices for multi-class classification. decode ( self , y ) Decode a collection of class indices. Parameters: Name Type Description Default y ndarray Labels as (multilabel) one-hot encodings required Returns: Type Description List[List[str]] List of original labels for each output. Source code in tagifai/data.py def decode ( self , y : np . ndarray ) -> List [ List [ str ]]: \"\"\"Decode a collection of class indices. Args: y (np.ndarray): Labels as (multilabel) one-hot encodings Returns: List of original labels for each output. \"\"\" classes = [] for i , item in enumerate ( y ): classes . append ( self . index_to_class [ item ]) return classes encode ( self , y ) Encode a collection of classes. Parameters: Name Type Description Default y Series Collection of labels as a pandas Series object. required Returns: Type Description ndarray Labels as (multilabel) one-hot encodings Source code in tagifai/data.py def encode ( self , y : pd . Series ) -> np . ndarray : \"\"\"Encode a collection of classes. Args: y (pd.Series): Collection of labels as a pandas Series object. Returns: Labels as (multilabel) one-hot encodings \"\"\" encoded = np . zeros (( len ( y )), dtype = int ) for i , item in enumerate ( y ): encoded [ i ] = self . class_to_index [ item ] return encoded fit ( self , y ) Learn label mappings from a series of class labels. Parameters: Name Type Description Default y Sequence Collection of labels as a pandas Series object. required Source code in tagifai/data.py def fit ( self , y : Sequence ): \"\"\"Learn label mappings from a series of class labels. Args: y (Sequence): Collection of labels as a pandas Series object. \"\"\" classes = np . unique ( y ) for i , class_ in enumerate ( classes ): self . class_to_index [ class_ ] = i self . index_to_class = { v : k for k , v in self . class_to_index . items ()} self . classes = list ( self . class_to_index . keys ()) return self MultiLabelLabelEncoder Encode labels into unique indices for multi-label classification. decode ( self , y ) Decode a (multilabel) one-hot encoding into corresponding labels. Parameters: Name Type Description Default y ndarray Labels as (multilabel) one-hot encodings required Returns: Type Description List[List[str]] List of original labels for each output. Source code in tagifai/data.py def decode ( self , y : np . ndarray ) -> List [ List [ str ]]: \"\"\"Decode a (multilabel) one-hot encoding into corresponding labels. Args: y (np.ndarray): Labels as (multilabel) one-hot encodings Returns: List of original labels for each output. \"\"\" classes = [] for i , item in enumerate ( y ): indices = np . where ( np . asarray ( item ) == 1 )[ 0 ] classes . append ([ self . index_to_class [ index ] for index in indices ]) return classes encode ( self , y ) Encode a collection of labels using (multilabel) one-hot encoding. Parameters: Name Type Description Default y Series Collection of labels as a pandas Series object. required Returns: Type Description ndarray Labels as (multilabel) one-hot encodings Source code in tagifai/data.py def encode ( self , y : pd . Series ) -> np . ndarray : \"\"\"Encode a collection of labels using (multilabel) one-hot encoding. Args: y (pd.Series): Collection of labels as a pandas Series object. Returns: Labels as (multilabel) one-hot encodings \"\"\" y_one_hot = np . zeros (( len ( y ), len ( self . class_to_index )), dtype = int ) for i , item in enumerate ( y ): for class_ in item : y_one_hot [ i ][ self . class_to_index [ class_ ]] = 1 return y_one_hot fit ( self , y ) Learn label mappings from a series of class labels. Parameters: Name Type Description Default y Sequence Collection of labels as a pandas Series object. required Source code in tagifai/data.py def fit ( self , y : Sequence ): \"\"\"Learn label mappings from a series of class labels. Args: y (Sequence): Collection of labels as a pandas Series object. \"\"\" classes = np . unique ( list ( itertools . chain . from_iterable ( y ))) for i , class_ in enumerate ( classes ): self . class_to_index [ class_ ] = i self . index_to_class = { v : k for k , v in self . class_to_index . items ()} self . classes = list ( self . class_to_index . keys ()) return self Stemmer stem ( self , word ) Strip affixes from the token and return the stem. :param token: The token that should be stemmed. :type token: str Source code in tagifai/data.py def stem ( self , word ): if self . mode == self . NLTK_EXTENSIONS and word in self . pool : # pragma: no cover, nltk return self . pool [ word ] if self . mode != self . ORIGINAL_ALGORITHM and len ( word ) <= 2 : # pragma: no cover, nltk # With this line, strings of length 1 or 2 don't go through # the stemming process, although no mention is made of this # in the published algorithm. return word stem = self . _step1a ( word ) stem = self . _step1b ( stem ) stem = self . _step1c ( stem ) stem = self . _step2 ( stem ) stem = self . _step3 ( stem ) stem = self . _step4 ( stem ) stem = self . _step5a ( stem ) stem = self . _step5b ( stem ) return stem Tokenizer Tokenize a feature using a built vocabulary. Usage: tokenizer = Tokenizer ( char_level = char_level ) tokenizer . fit_on_texts ( texts = X ) X = np . array ( tokenizer . texts_to_sequences ( X ), dtype = object ) fit_on_texts ( self , texts ) Learn token mappings from a list of texts. Parameters: Name Type Description Default texts List List of texts made of tokens. required Source code in tagifai/data.py def fit_on_texts ( self , texts : List ): \"\"\"Learn token mappings from a list of texts. Args: texts (List): List of texts made of tokens. \"\"\" if not self . char_level : texts = [ text . split ( \" \" ) for text in texts ] all_tokens = [ token for text in texts for token in text ] counts = Counter ( all_tokens ) . most_common ( self . num_tokens ) self . min_token_freq = counts [ - 1 ][ 1 ] for token , count in counts : index = len ( self ) self . token_to_index [ token ] = index self . index_to_token [ index ] = token return self sequences_to_texts ( self , sequences ) Convert a lists of arrays of indices to a list of texts. Parameters: Name Type Description Default sequences List list of mapped tokens to convert back to text. required Returns: Type Description List Mapped text from index tokens. Source code in tagifai/data.py def sequences_to_texts ( self , sequences : List ) -> List : \"\"\"Convert a lists of arrays of indices to a list of texts. Args: sequences (List): list of mapped tokens to convert back to text. Returns: Mapped text from index tokens. \"\"\" texts = [] for sequence in sequences : text = [] for index in sequence : text . append ( self . index_to_token . get ( index , self . oov_token )) texts . append ( self . separator . join ([ token for token in text ])) return texts texts_to_sequences ( self , texts ) Convert a list of texts to a lists of arrays of indices. Parameters: Name Type Description Default texts List List of texts to tokenize and map to indices. required Returns: Type Description List[List] A list of mapped sequences (list of indices). Source code in tagifai/data.py def texts_to_sequences ( self , texts : List ) -> List [ List ]: \"\"\"Convert a list of texts to a lists of arrays of indices. Args: texts (List): List of texts to tokenize and map to indices. Returns: A list of mapped sequences (list of indices). \"\"\" sequences = [] for text in texts : if not self . char_level : text = text . split ( \" \" ) sequence = [] for token in text : sequence . append ( self . token_to_index . get ( token , self . token_to_index [ self . oov_token ])) sequences . append ( sequence ) return sequences compute_features ( params ) Compute features to use for training. Parameters: Name Type Description Default params Namespace Input parameters for operations. required Source code in tagifai/data.py def compute_features ( params : Namespace ) -> None : \"\"\"Compute features to use for training. Args: params (Namespace): Input parameters for operations. \"\"\" # Set up utils . set_seed ( seed = params . seed ) # Load data projects = utils . load_dict ( filepath = Path ( config . DATA_DIR , \"projects.json\" )) df = pd . DataFrame ( projects ) # Compute features df [ \"text\" ] = df . title + \" \" + df . description df . drop ( columns = [ \"title\" , \"description\" ], inplace = True ) df = df [[ \"id\" , \"created_on\" , \"text\" , \"tags\" ]] # Save features = df . to_dict ( orient = \"records\" ) df_dict_fp = Path ( config . DATA_DIR , \"features.json\" ) utils . save_dict ( d = features , filepath = df_dict_fp ) return df , features filter_items ( items , include = [], exclude = []) Filter a list using inclusion and exclusion lists of items. Parameters: Name Type Description Default items List List of items to apply filters. required include List List of items to include. Defaults to []. [] exclude List List of items to filter out. Defaults to []. [] Returns: Type Description List Filtered list of items. Usage: # Filter tags for each project df . tags = df . tags . apply ( filter_items , include = list ( tags_dict . keys ()), exclude = config . EXCLUDE , ) Source code in tagifai/data.py def filter_items ( items : List , include : List = [], exclude : List = []) -> List : \"\"\"Filter a list using inclusion and exclusion lists of items. Args: items (List): List of items to apply filters. include (List, optional): List of items to include. Defaults to []. exclude (List, optional): List of items to filter out. Defaults to []. Returns: Filtered list of items. Usage: ```python # Filter tags for each project df.tags = df.tags.apply( filter_items, include=list(tags_dict.keys()), exclude=config.EXCLUDE, ) ``` \"\"\" # Filter filtered = [ item for item in items if item in include and item not in exclude ] return filtered iterative_train_test_split ( X , y , train_size = 0.7 ) Custom iterative train test split which 'maintains balanced representation with respect to order-th label combinations.' Parameters: Name Type Description Default X Series Input features as a pandas Series object. required y ndarray One-hot encoded labels. required train_size float Proportion of data for first split. Defaults to 0.7. 0.7 Returns: Type Description Tuple Two stratified splits based on specified proportions. Source code in tagifai/data.py def iterative_train_test_split ( X : pd . Series , y : np . ndarray , train_size : float = 0.7 ) -> Tuple : \"\"\"Custom iterative train test split which 'maintains balanced representation with respect to order-th label combinations.' Args: X (pd.Series): Input features as a pandas Series object. y (np.ndarray): One-hot encoded labels. train_size (float, optional): Proportion of data for first split. Defaults to 0.7. Returns: Two stratified splits based on specified proportions. \"\"\" stratifier = IterativeStratification ( n_splits = 2 , order = 1 , sample_distribution_per_fold = [ 1.0 - train_size , train_size , ], ) train_indices , test_indices = next ( stratifier . split ( X , y )) X_train , y_train = X [ train_indices ], y [ train_indices ] X_test , y_test = X [ test_indices ], y [ test_indices ] return X_train , X_test , y_train , y_test pad_sequences ( sequences , max_seq_len = 0 ) Zero pad sequences to a specified max_seq_len or to the length of the largest sequence in sequences . Usage: # Pad inputs seq = np . array ([[ 1 , 2 , 3 ], [ 1 , 2 ]], dtype = object ) padded_seq = pad_sequences ( sequences = seq , max_seq_len = 5 ) print ( padded_seq ) [[1. 2. 3. 0. 0.] [1. 2. 0. 0. 0.]] Note Input sequences must be 2D. Check out this implemention {:target=\"_blank\"} for a more generalized approach. Parameters: Name Type Description Default sequences ndarray 2D array of data to be padded. required max_seq_len int Length to pad sequences to. Defaults to 0. 0 Exceptions: Type Description ValueError Input sequences are not two-dimensional. Returns: Type Description ndarray An array with the zero padded sequences. Source code in tagifai/data.py def pad_sequences ( sequences : np . ndarray , max_seq_len : int = 0 ) -> np . ndarray : \"\"\"Zero pad sequences to a specified `max_seq_len` or to the length of the largest sequence in `sequences`. Usage: ```python # Pad inputs seq = np.array([[1, 2, 3], [1, 2]], dtype=object) padded_seq = pad_sequences(sequences=seq, max_seq_len=5) print (padded_seq) ``` <pre> [[1. 2. 3. 0. 0.] [1. 2. 0. 0. 0.]] </pre> Note: Input `sequences` must be 2D. Check out this [implemention](https://madewithml.com/courses/foundations/convolutional-neural-networks/#padding){:target=\"_blank\"} for a more generalized approach. Args: sequences (np.ndarray): 2D array of data to be padded. max_seq_len (int, optional): Length to pad sequences to. Defaults to 0. Raises: ValueError: Input sequences are not two-dimensional. Returns: An array with the zero padded sequences. \"\"\" # Get max sequence length max_seq_len = max ( max_seq_len , max ( len ( sequence ) for sequence in sequences )) # Pad padded_sequences = np . zeros (( len ( sequences ), max_seq_len )) for i , sequence in enumerate ( sequences ): padded_sequences [ i ][: len ( sequence )] = sequence return padded_sequences prepare ( df , include = [], exclude = [], min_tag_freq = 30 ) Prepare the raw data. Parameters: Name Type Description Default df DataFrame Pandas DataFrame with data. required include List list of tags to include. [] exclude List list of tags to exclude. [] min_tag_freq int Minimum frequency of tags required. Defaults to 30. 30 Returns: Type Description Tuple A cleaned dataframe and dictionary of tags and counts above the frequency threshold. Source code in tagifai/data.py def prepare ( df : pd . DataFrame , include : List = [], exclude : List = [], min_tag_freq : int = 30 ) -> Tuple : \"\"\"Prepare the raw data. Args: df (pd.DataFrame): Pandas DataFrame with data. include (List): list of tags to include. exclude (List): list of tags to exclude. min_tag_freq (int, optional): Minimum frequency of tags required. Defaults to 30. Returns: A cleaned dataframe and dictionary of tags and counts above the frequency threshold. \"\"\" # Filter tags for each project df . tags = df . tags . apply ( filter_items , include = include , exclude = exclude ) tags = Counter ( itertools . chain . from_iterable ( df . tags . values )) # Filter tags that have fewer than `min_tag_freq` occurrences tags_above_freq = Counter ( tag for tag in tags . elements () if tags [ tag ] >= min_tag_freq ) tags_below_freq = Counter ( tag for tag in tags . elements () if tags [ tag ] < min_tag_freq ) df . tags = df . tags . apply ( filter_items , include = list ( tags_above_freq . keys ())) # Remove projects with no more remaining relevant tags df = df [ df . tags . map ( len ) > 0 ] return df , tags_above_freq , tags_below_freq preprocess ( text , lower = True , stem = False , stopwords = [ 'i' , 'me' , 'my' , 'myself' , 'we' , 'our' , 'ours' , 'ourselves' , 'you' , \"you're\" , \"you've\" , \"you'll\" , \"you'd\" , 'your' , 'yours' , 'yourself' , 'yourselves' , 'he' , 'him' , 'his' , 'himself' , 'she' , \"she's\" , 'her' , 'hers' , 'herself' , 'it' , \"it's\" , 'its' , 'itself' , 'they' , 'them' , 'their' , 'theirs' , 'themselves' , 'what' , 'which' , 'who' , 'whom' , 'this' , 'that' , \"that'll\" , 'these' , 'those' , 'am' , 'is' , 'are' , 'was' , 'were' , 'be' , 'been' , 'being' , 'have' , 'has' , 'had' , 'having' , 'do' , 'does' , 'did' , 'doing' , 'a' , 'an' , 'the' , 'and' , 'but' , 'if' , 'or' , 'because' , 'as' , 'until' , 'while' , 'of' , 'at' , 'by' , 'for' , 'with' , 'about' , 'against' , 'between' , 'into' , 'through' , 'during' , 'before' , 'after' , 'above' , 'below' , 'to' , 'from' , 'up' , 'down' , 'in' , 'out' , 'on' , 'off' , 'over' , 'under' , 'again' , 'further' , 'then' , 'once' , 'here' , 'there' , 'when' , 'where' , 'why' , 'how' , 'all' , 'any' , 'both' , 'each' , 'few' , 'more' , 'most' , 'other' , 'some' , 'such' , 'no' , 'nor' , 'not' , 'only' , 'own' , 'same' , 'so' , 'than' , 'too' , 'very' , 's' , 't' , 'can' , 'will' , 'just' , 'don' , \"don't\" , 'should' , \"should've\" , 'now' , 'd' , 'll' , 'm' , 'o' , 're' , 've' , 'y' , 'ain' , 'aren' , \"aren't\" , 'couldn' , \"couldn't\" , 'didn' , \"didn't\" , 'doesn' , \"doesn't\" , 'hadn' , \"hadn't\" , 'hasn' , \"hasn't\" , 'haven' , \"haven't\" , 'isn' , \"isn't\" , 'ma' , 'mightn' , \"mightn't\" , 'mustn' , \"mustn't\" , 'needn' , \"needn't\" , 'shan' , \"shan't\" , 'shouldn' , \"shouldn't\" , 'wasn' , \"wasn't\" , 'weren' , \"weren't\" , 'won' , \"won't\" , 'wouldn' , \"wouldn't\" ]) Conditional preprocessing on text. Usage: preprocess ( text = \"Transfer learning with BERT!\" , lower = True , stem = True ) 'transfer learn bert' Parameters: Name Type Description Default text str String to preprocess. required lower bool Lower the text. Defaults to True. True stem bool Stem the text. Defaults to False. False stopwords List List of words to filter out. Defaults to STOPWORDS. ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] Returns: Type Description str Preprocessed string. Source code in tagifai/data.py def preprocess ( text : str , lower : bool = True , stem : bool = False , stopwords : List = config . STOPWORDS , ) -> str : \"\"\"Conditional preprocessing on text. Usage: ```python preprocess(text=\"Transfer learning with BERT!\", lower=True, stem=True) ``` <pre> 'transfer learn bert' </pre> Args: text (str): String to preprocess. lower (bool, optional): Lower the text. Defaults to True. stem (bool, optional): Stem the text. Defaults to False. stopwords (List, optional): List of words to filter out. Defaults to STOPWORDS. Returns: Preprocessed string. \"\"\" # Lower if lower : text = text . lower () # Remove stopwords if len ( stopwords ): pattern = re . compile ( r \"\\b(\" + r \"|\" . join ( stopwords ) + r \")\\b\\s*\" ) text = pattern . sub ( \"\" , text ) # Spacing and filters text = re . sub ( r \"([! \\\" '#$%&()*\\+,-./:;<=>?@ \\\\ \\[\\]^_`{|}~])\" , r \" \\1 \" , text ) # add spacing between objects to be filtered text = re . sub ( \"[^A-Za-z0-9]+\" , \" \" , text ) # remove non alphanumeric chars text = re . sub ( \" +\" , \" \" , text ) # remove multiple spaces text = text . strip () # Remove links text = re . sub ( r \"http\\S+\" , \"\" , text ) # Stemming if stem : stemmer = Stemmer () text = \" \" . join ([ stemmer . stem ( word ) for word in text . split ( \" \" )]) return text","title":"Data"},{"location":"tagifai/data/#tagifai.data.CNNTextDataset","text":"Create torch.utils.data.Dataset objects to use for efficiently feeding data into our models. Usage: # Create dataset X , y = data dataset = CNNTextDataset ( X = X , y = y , max_filter_size = max_filter_size ) # Create dataloaders dataloader = dataset . create_dataloader ( batch_size = batch_size )","title":"CNNTextDataset"},{"location":"tagifai/data/#tagifai.data.CNNTextDataset.collate_fn","text":"Processing on a batch. It's used to override the default collate_fn in torch.utils.data.DataLoader . Parameters: Name Type Description Default batch List List of inputs and outputs. required Returns: Type Description Tuple Processed inputs and outputs. Source code in tagifai/data.py def collate_fn ( self , batch : List ) -> Tuple : \"\"\"Processing on a batch. It's used to override the default `collate_fn` in `torch.utils.data.DataLoader`. Args: batch (List): List of inputs and outputs. Returns: Processed inputs and outputs. \"\"\" # Get inputs batch = np . array ( batch , dtype = object ) X = batch [:, 0 ] y = np . stack ( batch [:, 1 ], axis = 0 ) # Pad inputs X = pad_sequences ( sequences = X , max_seq_len = self . max_filter_size ) # Cast X = torch . LongTensor ( X . astype ( np . int32 )) y = torch . FloatTensor ( y . astype ( np . int32 )) return X , y","title":"collate_fn()"},{"location":"tagifai/data/#tagifai.data.CNNTextDataset.create_dataloader","text":"Create dataloaders to load batches with. Usage: # Create dataset X , y = data dataset = CNNTextDataset ( X = X , y = y , max_filter_size = max_filter_size ) # Create dataloaders dataloader = dataset . create_dataloader ( batch_size = batch_size ) Parameters: Name Type Description Default batch_size int Number of samples per batch. required shuffle bool Shuffle each batch. Defaults to False. False drop_last bool Drop the last batch if it's less than batch_size . Defaults to False. False Returns: Type Description DataLoader Torch dataloader to load batches with. Source code in tagifai/data.py def create_dataloader ( self , batch_size : int , shuffle : bool = False , drop_last : bool = False ) -> torch . utils . data . DataLoader : \"\"\"Create dataloaders to load batches with. Usage: ```python # Create dataset X, y = data dataset = CNNTextDataset(X=X, y=y, max_filter_size=max_filter_size) # Create dataloaders dataloader = dataset.create_dataloader(batch_size=batch_size) ``` Args: batch_size (int): Number of samples per batch. shuffle (bool, optional): Shuffle each batch. Defaults to False. drop_last (bool, optional): Drop the last batch if it's less than `batch_size`. Defaults to False. Returns: Torch dataloader to load batches with. \"\"\" return torch . utils . data . DataLoader ( dataset = self , batch_size = batch_size , collate_fn = self . collate_fn , shuffle = shuffle , drop_last = drop_last , pin_memory = True , )","title":"create_dataloader()"},{"location":"tagifai/data/#tagifai.data.LabelEncoder","text":"Encode labels into unique indices. Usage: # Encode labels label_encoder = LabelEncoder () label_encoder . fit ( labels ) y = label_encoder . encode ( labels )","title":"LabelEncoder"},{"location":"tagifai/data/#tagifai.data.MultiClassLabelEncoder","text":"Encode labels into unique indices for multi-class classification.","title":"MultiClassLabelEncoder"},{"location":"tagifai/data/#tagifai.data.MultiClassLabelEncoder.decode","text":"Decode a collection of class indices. Parameters: Name Type Description Default y ndarray Labels as (multilabel) one-hot encodings required Returns: Type Description List[List[str]] List of original labels for each output. Source code in tagifai/data.py def decode ( self , y : np . ndarray ) -> List [ List [ str ]]: \"\"\"Decode a collection of class indices. Args: y (np.ndarray): Labels as (multilabel) one-hot encodings Returns: List of original labels for each output. \"\"\" classes = [] for i , item in enumerate ( y ): classes . append ( self . index_to_class [ item ]) return classes","title":"decode()"},{"location":"tagifai/data/#tagifai.data.MultiClassLabelEncoder.encode","text":"Encode a collection of classes. Parameters: Name Type Description Default y Series Collection of labels as a pandas Series object. required Returns: Type Description ndarray Labels as (multilabel) one-hot encodings Source code in tagifai/data.py def encode ( self , y : pd . Series ) -> np . ndarray : \"\"\"Encode a collection of classes. Args: y (pd.Series): Collection of labels as a pandas Series object. Returns: Labels as (multilabel) one-hot encodings \"\"\" encoded = np . zeros (( len ( y )), dtype = int ) for i , item in enumerate ( y ): encoded [ i ] = self . class_to_index [ item ] return encoded","title":"encode()"},{"location":"tagifai/data/#tagifai.data.MultiClassLabelEncoder.fit","text":"Learn label mappings from a series of class labels. Parameters: Name Type Description Default y Sequence Collection of labels as a pandas Series object. required Source code in tagifai/data.py def fit ( self , y : Sequence ): \"\"\"Learn label mappings from a series of class labels. Args: y (Sequence): Collection of labels as a pandas Series object. \"\"\" classes = np . unique ( y ) for i , class_ in enumerate ( classes ): self . class_to_index [ class_ ] = i self . index_to_class = { v : k for k , v in self . class_to_index . items ()} self . classes = list ( self . class_to_index . keys ()) return self","title":"fit()"},{"location":"tagifai/data/#tagifai.data.MultiLabelLabelEncoder","text":"Encode labels into unique indices for multi-label classification.","title":"MultiLabelLabelEncoder"},{"location":"tagifai/data/#tagifai.data.MultiLabelLabelEncoder.decode","text":"Decode a (multilabel) one-hot encoding into corresponding labels. Parameters: Name Type Description Default y ndarray Labels as (multilabel) one-hot encodings required Returns: Type Description List[List[str]] List of original labels for each output. Source code in tagifai/data.py def decode ( self , y : np . ndarray ) -> List [ List [ str ]]: \"\"\"Decode a (multilabel) one-hot encoding into corresponding labels. Args: y (np.ndarray): Labels as (multilabel) one-hot encodings Returns: List of original labels for each output. \"\"\" classes = [] for i , item in enumerate ( y ): indices = np . where ( np . asarray ( item ) == 1 )[ 0 ] classes . append ([ self . index_to_class [ index ] for index in indices ]) return classes","title":"decode()"},{"location":"tagifai/data/#tagifai.data.MultiLabelLabelEncoder.encode","text":"Encode a collection of labels using (multilabel) one-hot encoding. Parameters: Name Type Description Default y Series Collection of labels as a pandas Series object. required Returns: Type Description ndarray Labels as (multilabel) one-hot encodings Source code in tagifai/data.py def encode ( self , y : pd . Series ) -> np . ndarray : \"\"\"Encode a collection of labels using (multilabel) one-hot encoding. Args: y (pd.Series): Collection of labels as a pandas Series object. Returns: Labels as (multilabel) one-hot encodings \"\"\" y_one_hot = np . zeros (( len ( y ), len ( self . class_to_index )), dtype = int ) for i , item in enumerate ( y ): for class_ in item : y_one_hot [ i ][ self . class_to_index [ class_ ]] = 1 return y_one_hot","title":"encode()"},{"location":"tagifai/data/#tagifai.data.MultiLabelLabelEncoder.fit","text":"Learn label mappings from a series of class labels. Parameters: Name Type Description Default y Sequence Collection of labels as a pandas Series object. required Source code in tagifai/data.py def fit ( self , y : Sequence ): \"\"\"Learn label mappings from a series of class labels. Args: y (Sequence): Collection of labels as a pandas Series object. \"\"\" classes = np . unique ( list ( itertools . chain . from_iterable ( y ))) for i , class_ in enumerate ( classes ): self . class_to_index [ class_ ] = i self . index_to_class = { v : k for k , v in self . class_to_index . items ()} self . classes = list ( self . class_to_index . keys ()) return self","title":"fit()"},{"location":"tagifai/data/#tagifai.data.Stemmer","text":"","title":"Stemmer"},{"location":"tagifai/data/#tagifai.data.Stemmer.stem","text":"Strip affixes from the token and return the stem. :param token: The token that should be stemmed. :type token: str Source code in tagifai/data.py def stem ( self , word ): if self . mode == self . NLTK_EXTENSIONS and word in self . pool : # pragma: no cover, nltk return self . pool [ word ] if self . mode != self . ORIGINAL_ALGORITHM and len ( word ) <= 2 : # pragma: no cover, nltk # With this line, strings of length 1 or 2 don't go through # the stemming process, although no mention is made of this # in the published algorithm. return word stem = self . _step1a ( word ) stem = self . _step1b ( stem ) stem = self . _step1c ( stem ) stem = self . _step2 ( stem ) stem = self . _step3 ( stem ) stem = self . _step4 ( stem ) stem = self . _step5a ( stem ) stem = self . _step5b ( stem ) return stem","title":"stem()"},{"location":"tagifai/data/#tagifai.data.Tokenizer","text":"Tokenize a feature using a built vocabulary. Usage: tokenizer = Tokenizer ( char_level = char_level ) tokenizer . fit_on_texts ( texts = X ) X = np . array ( tokenizer . texts_to_sequences ( X ), dtype = object )","title":"Tokenizer"},{"location":"tagifai/data/#tagifai.data.Tokenizer.fit_on_texts","text":"Learn token mappings from a list of texts. Parameters: Name Type Description Default texts List List of texts made of tokens. required Source code in tagifai/data.py def fit_on_texts ( self , texts : List ): \"\"\"Learn token mappings from a list of texts. Args: texts (List): List of texts made of tokens. \"\"\" if not self . char_level : texts = [ text . split ( \" \" ) for text in texts ] all_tokens = [ token for text in texts for token in text ] counts = Counter ( all_tokens ) . most_common ( self . num_tokens ) self . min_token_freq = counts [ - 1 ][ 1 ] for token , count in counts : index = len ( self ) self . token_to_index [ token ] = index self . index_to_token [ index ] = token return self","title":"fit_on_texts()"},{"location":"tagifai/data/#tagifai.data.Tokenizer.sequences_to_texts","text":"Convert a lists of arrays of indices to a list of texts. Parameters: Name Type Description Default sequences List list of mapped tokens to convert back to text. required Returns: Type Description List Mapped text from index tokens. Source code in tagifai/data.py def sequences_to_texts ( self , sequences : List ) -> List : \"\"\"Convert a lists of arrays of indices to a list of texts. Args: sequences (List): list of mapped tokens to convert back to text. Returns: Mapped text from index tokens. \"\"\" texts = [] for sequence in sequences : text = [] for index in sequence : text . append ( self . index_to_token . get ( index , self . oov_token )) texts . append ( self . separator . join ([ token for token in text ])) return texts","title":"sequences_to_texts()"},{"location":"tagifai/data/#tagifai.data.Tokenizer.texts_to_sequences","text":"Convert a list of texts to a lists of arrays of indices. Parameters: Name Type Description Default texts List List of texts to tokenize and map to indices. required Returns: Type Description List[List] A list of mapped sequences (list of indices). Source code in tagifai/data.py def texts_to_sequences ( self , texts : List ) -> List [ List ]: \"\"\"Convert a list of texts to a lists of arrays of indices. Args: texts (List): List of texts to tokenize and map to indices. Returns: A list of mapped sequences (list of indices). \"\"\" sequences = [] for text in texts : if not self . char_level : text = text . split ( \" \" ) sequence = [] for token in text : sequence . append ( self . token_to_index . get ( token , self . token_to_index [ self . oov_token ])) sequences . append ( sequence ) return sequences","title":"texts_to_sequences()"},{"location":"tagifai/data/#tagifai.data.compute_features","text":"Compute features to use for training. Parameters: Name Type Description Default params Namespace Input parameters for operations. required Source code in tagifai/data.py def compute_features ( params : Namespace ) -> None : \"\"\"Compute features to use for training. Args: params (Namespace): Input parameters for operations. \"\"\" # Set up utils . set_seed ( seed = params . seed ) # Load data projects = utils . load_dict ( filepath = Path ( config . DATA_DIR , \"projects.json\" )) df = pd . DataFrame ( projects ) # Compute features df [ \"text\" ] = df . title + \" \" + df . description df . drop ( columns = [ \"title\" , \"description\" ], inplace = True ) df = df [[ \"id\" , \"created_on\" , \"text\" , \"tags\" ]] # Save features = df . to_dict ( orient = \"records\" ) df_dict_fp = Path ( config . DATA_DIR , \"features.json\" ) utils . save_dict ( d = features , filepath = df_dict_fp ) return df , features","title":"compute_features()"},{"location":"tagifai/data/#tagifai.data.filter_items","text":"Filter a list using inclusion and exclusion lists of items. Parameters: Name Type Description Default items List List of items to apply filters. required include List List of items to include. Defaults to []. [] exclude List List of items to filter out. Defaults to []. [] Returns: Type Description List Filtered list of items. Usage: # Filter tags for each project df . tags = df . tags . apply ( filter_items , include = list ( tags_dict . keys ()), exclude = config . EXCLUDE , ) Source code in tagifai/data.py def filter_items ( items : List , include : List = [], exclude : List = []) -> List : \"\"\"Filter a list using inclusion and exclusion lists of items. Args: items (List): List of items to apply filters. include (List, optional): List of items to include. Defaults to []. exclude (List, optional): List of items to filter out. Defaults to []. Returns: Filtered list of items. Usage: ```python # Filter tags for each project df.tags = df.tags.apply( filter_items, include=list(tags_dict.keys()), exclude=config.EXCLUDE, ) ``` \"\"\" # Filter filtered = [ item for item in items if item in include and item not in exclude ] return filtered","title":"filter_items()"},{"location":"tagifai/data/#tagifai.data.iterative_train_test_split","text":"Custom iterative train test split which 'maintains balanced representation with respect to order-th label combinations.' Parameters: Name Type Description Default X Series Input features as a pandas Series object. required y ndarray One-hot encoded labels. required train_size float Proportion of data for first split. Defaults to 0.7. 0.7 Returns: Type Description Tuple Two stratified splits based on specified proportions. Source code in tagifai/data.py def iterative_train_test_split ( X : pd . Series , y : np . ndarray , train_size : float = 0.7 ) -> Tuple : \"\"\"Custom iterative train test split which 'maintains balanced representation with respect to order-th label combinations.' Args: X (pd.Series): Input features as a pandas Series object. y (np.ndarray): One-hot encoded labels. train_size (float, optional): Proportion of data for first split. Defaults to 0.7. Returns: Two stratified splits based on specified proportions. \"\"\" stratifier = IterativeStratification ( n_splits = 2 , order = 1 , sample_distribution_per_fold = [ 1.0 - train_size , train_size , ], ) train_indices , test_indices = next ( stratifier . split ( X , y )) X_train , y_train = X [ train_indices ], y [ train_indices ] X_test , y_test = X [ test_indices ], y [ test_indices ] return X_train , X_test , y_train , y_test","title":"iterative_train_test_split()"},{"location":"tagifai/data/#tagifai.data.pad_sequences","text":"Zero pad sequences to a specified max_seq_len or to the length of the largest sequence in sequences . Usage: # Pad inputs seq = np . array ([[ 1 , 2 , 3 ], [ 1 , 2 ]], dtype = object ) padded_seq = pad_sequences ( sequences = seq , max_seq_len = 5 ) print ( padded_seq ) [[1. 2. 3. 0. 0.] [1. 2. 0. 0. 0.]] Note Input sequences must be 2D. Check out this implemention {:target=\"_blank\"} for a more generalized approach. Parameters: Name Type Description Default sequences ndarray 2D array of data to be padded. required max_seq_len int Length to pad sequences to. Defaults to 0. 0 Exceptions: Type Description ValueError Input sequences are not two-dimensional. Returns: Type Description ndarray An array with the zero padded sequences. Source code in tagifai/data.py def pad_sequences ( sequences : np . ndarray , max_seq_len : int = 0 ) -> np . ndarray : \"\"\"Zero pad sequences to a specified `max_seq_len` or to the length of the largest sequence in `sequences`. Usage: ```python # Pad inputs seq = np.array([[1, 2, 3], [1, 2]], dtype=object) padded_seq = pad_sequences(sequences=seq, max_seq_len=5) print (padded_seq) ``` <pre> [[1. 2. 3. 0. 0.] [1. 2. 0. 0. 0.]] </pre> Note: Input `sequences` must be 2D. Check out this [implemention](https://madewithml.com/courses/foundations/convolutional-neural-networks/#padding){:target=\"_blank\"} for a more generalized approach. Args: sequences (np.ndarray): 2D array of data to be padded. max_seq_len (int, optional): Length to pad sequences to. Defaults to 0. Raises: ValueError: Input sequences are not two-dimensional. Returns: An array with the zero padded sequences. \"\"\" # Get max sequence length max_seq_len = max ( max_seq_len , max ( len ( sequence ) for sequence in sequences )) # Pad padded_sequences = np . zeros (( len ( sequences ), max_seq_len )) for i , sequence in enumerate ( sequences ): padded_sequences [ i ][: len ( sequence )] = sequence return padded_sequences","title":"pad_sequences()"},{"location":"tagifai/data/#tagifai.data.prepare","text":"Prepare the raw data. Parameters: Name Type Description Default df DataFrame Pandas DataFrame with data. required include List list of tags to include. [] exclude List list of tags to exclude. [] min_tag_freq int Minimum frequency of tags required. Defaults to 30. 30 Returns: Type Description Tuple A cleaned dataframe and dictionary of tags and counts above the frequency threshold. Source code in tagifai/data.py def prepare ( df : pd . DataFrame , include : List = [], exclude : List = [], min_tag_freq : int = 30 ) -> Tuple : \"\"\"Prepare the raw data. Args: df (pd.DataFrame): Pandas DataFrame with data. include (List): list of tags to include. exclude (List): list of tags to exclude. min_tag_freq (int, optional): Minimum frequency of tags required. Defaults to 30. Returns: A cleaned dataframe and dictionary of tags and counts above the frequency threshold. \"\"\" # Filter tags for each project df . tags = df . tags . apply ( filter_items , include = include , exclude = exclude ) tags = Counter ( itertools . chain . from_iterable ( df . tags . values )) # Filter tags that have fewer than `min_tag_freq` occurrences tags_above_freq = Counter ( tag for tag in tags . elements () if tags [ tag ] >= min_tag_freq ) tags_below_freq = Counter ( tag for tag in tags . elements () if tags [ tag ] < min_tag_freq ) df . tags = df . tags . apply ( filter_items , include = list ( tags_above_freq . keys ())) # Remove projects with no more remaining relevant tags df = df [ df . tags . map ( len ) > 0 ] return df , tags_above_freq , tags_below_freq","title":"prepare()"},{"location":"tagifai/data/#tagifai.data.preprocess","text":"Conditional preprocessing on text. Usage: preprocess ( text = \"Transfer learning with BERT!\" , lower = True , stem = True ) 'transfer learn bert' Parameters: Name Type Description Default text str String to preprocess. required lower bool Lower the text. Defaults to True. True stem bool Stem the text. Defaults to False. False stopwords List List of words to filter out. Defaults to STOPWORDS. ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] Returns: Type Description str Preprocessed string. Source code in tagifai/data.py def preprocess ( text : str , lower : bool = True , stem : bool = False , stopwords : List = config . STOPWORDS , ) -> str : \"\"\"Conditional preprocessing on text. Usage: ```python preprocess(text=\"Transfer learning with BERT!\", lower=True, stem=True) ``` <pre> 'transfer learn bert' </pre> Args: text (str): String to preprocess. lower (bool, optional): Lower the text. Defaults to True. stem (bool, optional): Stem the text. Defaults to False. stopwords (List, optional): List of words to filter out. Defaults to STOPWORDS. Returns: Preprocessed string. \"\"\" # Lower if lower : text = text . lower () # Remove stopwords if len ( stopwords ): pattern = re . compile ( r \"\\b(\" + r \"|\" . join ( stopwords ) + r \")\\b\\s*\" ) text = pattern . sub ( \"\" , text ) # Spacing and filters text = re . sub ( r \"([! \\\" '#$%&()*\\+,-./:;<=>?@ \\\\ \\[\\]^_`{|}~])\" , r \" \\1 \" , text ) # add spacing between objects to be filtered text = re . sub ( \"[^A-Za-z0-9]+\" , \" \" , text ) # remove non alphanumeric chars text = re . sub ( \" +\" , \" \" , text ) # remove multiple spaces text = text . strip () # Remove links text = re . sub ( r \"http\\S+\" , \"\" , text ) # Stemming if stem : stemmer = Stemmer () text = \" \" . join ([ stemmer . stem ( word ) for word in text . split ( \" \" )]) return text","title":"preprocess()"},{"location":"tagifai/eval/","text":"evaluate ( df , artifacts , device = device ( type = 'cpu' )) Evaluate performance on data. Parameters: Name Type Description Default df DataFrame Dataframe (used for slicing). required artifacts Dict Artifacts needed for inference. required device device Device to run model on. Defaults to CPU. device(type='cpu') Returns: Type Description Tuple Ground truth and predicted labels, performance. Source code in tagifai/eval.py def evaluate ( df : pd . DataFrame , artifacts : Dict , device : torch . device = torch . device ( \"cpu\" ) ) -> Tuple : \"\"\"Evaluate performance on data. Args: df (pd.DataFrame): Dataframe (used for slicing). artifacts (Dict): Artifacts needed for inference. device (torch.device): Device to run model on. Defaults to CPU. Returns: Ground truth and predicted labels, performance. \"\"\" # Artifacts params = artifacts [ \"params\" ] model = artifacts [ \"model\" ] tokenizer = artifacts [ \"tokenizer\" ] label_encoder = artifacts [ \"label_encoder\" ] model = model . to ( device ) classes = label_encoder . classes # Create dataloader X = np . array ( tokenizer . texts_to_sequences ( df . text . to_numpy ()), dtype = \"object\" ) y = label_encoder . encode ( df . tags ) dataset = data . CNNTextDataset ( X = X , y = y , max_filter_size = int ( params . max_filter_size )) dataloader = dataset . create_dataloader ( batch_size = int ( params . batch_size )) # Determine predictions using threshold trainer = train . Trainer ( model = model , device = device ) y_true , y_prob = trainer . predict_step ( dataloader = dataloader ) y_pred = np . array ([ np . where ( prob >= float ( params . threshold ), 1 , 0 ) for prob in y_prob ]) # Evaluate performance performance = {} performance = get_metrics ( df = df , y_true = y_true , y_pred = y_pred , classes = classes ) return y_true , y_pred , performance get_metrics ( y_true , y_pred , classes , df = None ) Calculate metrics for fine-grained performance evaluation. Parameters: Name Type Description Default y_true ndarray True class labels. required y_pred ndarray Predicted class labels. required classes List List of all unique classes. required df DataFrame dataframe used for slicing. None Returns: Type Description Dict Dictionary of fine-grained performance metrics. Source code in tagifai/eval.py def get_metrics ( y_true : np . ndarray , y_pred : np . ndarray , classes : List , df : pd . DataFrame = None ) -> Dict : \"\"\"Calculate metrics for fine-grained performance evaluation. Args: y_true (np.ndarray): True class labels. y_pred (np.ndarray): Predicted class labels. classes (List): List of all unique classes. df (pd.DataFrame, optional): dataframe used for slicing. Returns: Dictionary of fine-grained performance metrics. \"\"\" # Performance metrics = { \"overall\" : {}, \"class\" : {}} # Overall metrics overall_metrics = precision_recall_fscore_support ( y_true , y_pred , average = \"weighted\" ) metrics [ \"overall\" ][ \"precision\" ] = overall_metrics [ 0 ] metrics [ \"overall\" ][ \"recall\" ] = overall_metrics [ 1 ] metrics [ \"overall\" ][ \"f1\" ] = overall_metrics [ 2 ] metrics [ \"overall\" ][ \"num_samples\" ] = np . float64 ( len ( y_true )) # Per-class metrics class_metrics = precision_recall_fscore_support ( y_true , y_pred , average = None ) for i in range ( len ( classes )): metrics [ \"class\" ][ classes [ i ]] = { \"precision\" : class_metrics [ 0 ][ i ], \"recall\" : class_metrics [ 1 ][ i ], \"f1\" : class_metrics [ 2 ][ i ], \"num_samples\" : np . float64 ( class_metrics [ 3 ][ i ]), } # Slicing metrics if df is not None : # Slices slicing_functions = [ cv_transformers , short_text ] applier = PandasSFApplier ( slicing_functions ) slices = applier . apply ( df ) # Score slices # Use snorkel.analysis.Scorer for multiclass tasks # Naive implementation for our multilabel task # based on snorkel.analysis.Scorer metrics [ \"slices\" ] = {} metrics [ \"slices\" ][ \"class\" ] = {} for slice_name in slices . dtype . names : mask = slices [ slice_name ] . astype ( bool ) if sum ( mask ): # pragma: no cover, test set may not have enough samples for slicing slice_metrics = precision_recall_fscore_support ( y_true [ mask ], y_pred [ mask ], average = \"micro\" ) metrics [ \"slices\" ][ \"class\" ][ slice_name ] = {} metrics [ \"slices\" ][ \"class\" ][ slice_name ][ \"precision\" ] = slice_metrics [ 0 ] metrics [ \"slices\" ][ \"class\" ][ slice_name ][ \"recall\" ] = slice_metrics [ 1 ] metrics [ \"slices\" ][ \"class\" ][ slice_name ][ \"f1\" ] = slice_metrics [ 2 ] metrics [ \"slices\" ][ \"class\" ][ slice_name ][ \"num_samples\" ] = len ( y_true [ mask ]) # Weighted overall slice metrics metrics [ \"slices\" ][ \"overall\" ] = {} for metric in [ \"precision\" , \"recall\" , \"f1\" ]: metrics [ \"slices\" ][ \"overall\" ][ metric ] = np . mean ( list ( itertools . chain . from_iterable ( [ [ metrics [ \"slices\" ][ \"class\" ][ slice_name ][ metric ]] * metrics [ \"slices\" ][ \"class\" ][ slice_name ][ \"num_samples\" ] for slice_name in metrics [ \"slices\" ][ \"class\" ] ] ) ) ) return metrics","title":"Eval"},{"location":"tagifai/eval/#tagifai.eval.evaluate","text":"Evaluate performance on data. Parameters: Name Type Description Default df DataFrame Dataframe (used for slicing). required artifacts Dict Artifacts needed for inference. required device device Device to run model on. Defaults to CPU. device(type='cpu') Returns: Type Description Tuple Ground truth and predicted labels, performance. Source code in tagifai/eval.py def evaluate ( df : pd . DataFrame , artifacts : Dict , device : torch . device = torch . device ( \"cpu\" ) ) -> Tuple : \"\"\"Evaluate performance on data. Args: df (pd.DataFrame): Dataframe (used for slicing). artifacts (Dict): Artifacts needed for inference. device (torch.device): Device to run model on. Defaults to CPU. Returns: Ground truth and predicted labels, performance. \"\"\" # Artifacts params = artifacts [ \"params\" ] model = artifacts [ \"model\" ] tokenizer = artifacts [ \"tokenizer\" ] label_encoder = artifacts [ \"label_encoder\" ] model = model . to ( device ) classes = label_encoder . classes # Create dataloader X = np . array ( tokenizer . texts_to_sequences ( df . text . to_numpy ()), dtype = \"object\" ) y = label_encoder . encode ( df . tags ) dataset = data . CNNTextDataset ( X = X , y = y , max_filter_size = int ( params . max_filter_size )) dataloader = dataset . create_dataloader ( batch_size = int ( params . batch_size )) # Determine predictions using threshold trainer = train . Trainer ( model = model , device = device ) y_true , y_prob = trainer . predict_step ( dataloader = dataloader ) y_pred = np . array ([ np . where ( prob >= float ( params . threshold ), 1 , 0 ) for prob in y_prob ]) # Evaluate performance performance = {} performance = get_metrics ( df = df , y_true = y_true , y_pred = y_pred , classes = classes ) return y_true , y_pred , performance","title":"evaluate()"},{"location":"tagifai/eval/#tagifai.eval.get_metrics","text":"Calculate metrics for fine-grained performance evaluation. Parameters: Name Type Description Default y_true ndarray True class labels. required y_pred ndarray Predicted class labels. required classes List List of all unique classes. required df DataFrame dataframe used for slicing. None Returns: Type Description Dict Dictionary of fine-grained performance metrics. Source code in tagifai/eval.py def get_metrics ( y_true : np . ndarray , y_pred : np . ndarray , classes : List , df : pd . DataFrame = None ) -> Dict : \"\"\"Calculate metrics for fine-grained performance evaluation. Args: y_true (np.ndarray): True class labels. y_pred (np.ndarray): Predicted class labels. classes (List): List of all unique classes. df (pd.DataFrame, optional): dataframe used for slicing. Returns: Dictionary of fine-grained performance metrics. \"\"\" # Performance metrics = { \"overall\" : {}, \"class\" : {}} # Overall metrics overall_metrics = precision_recall_fscore_support ( y_true , y_pred , average = \"weighted\" ) metrics [ \"overall\" ][ \"precision\" ] = overall_metrics [ 0 ] metrics [ \"overall\" ][ \"recall\" ] = overall_metrics [ 1 ] metrics [ \"overall\" ][ \"f1\" ] = overall_metrics [ 2 ] metrics [ \"overall\" ][ \"num_samples\" ] = np . float64 ( len ( y_true )) # Per-class metrics class_metrics = precision_recall_fscore_support ( y_true , y_pred , average = None ) for i in range ( len ( classes )): metrics [ \"class\" ][ classes [ i ]] = { \"precision\" : class_metrics [ 0 ][ i ], \"recall\" : class_metrics [ 1 ][ i ], \"f1\" : class_metrics [ 2 ][ i ], \"num_samples\" : np . float64 ( class_metrics [ 3 ][ i ]), } # Slicing metrics if df is not None : # Slices slicing_functions = [ cv_transformers , short_text ] applier = PandasSFApplier ( slicing_functions ) slices = applier . apply ( df ) # Score slices # Use snorkel.analysis.Scorer for multiclass tasks # Naive implementation for our multilabel task # based on snorkel.analysis.Scorer metrics [ \"slices\" ] = {} metrics [ \"slices\" ][ \"class\" ] = {} for slice_name in slices . dtype . names : mask = slices [ slice_name ] . astype ( bool ) if sum ( mask ): # pragma: no cover, test set may not have enough samples for slicing slice_metrics = precision_recall_fscore_support ( y_true [ mask ], y_pred [ mask ], average = \"micro\" ) metrics [ \"slices\" ][ \"class\" ][ slice_name ] = {} metrics [ \"slices\" ][ \"class\" ][ slice_name ][ \"precision\" ] = slice_metrics [ 0 ] metrics [ \"slices\" ][ \"class\" ][ slice_name ][ \"recall\" ] = slice_metrics [ 1 ] metrics [ \"slices\" ][ \"class\" ][ slice_name ][ \"f1\" ] = slice_metrics [ 2 ] metrics [ \"slices\" ][ \"class\" ][ slice_name ][ \"num_samples\" ] = len ( y_true [ mask ]) # Weighted overall slice metrics metrics [ \"slices\" ][ \"overall\" ] = {} for metric in [ \"precision\" , \"recall\" , \"f1\" ]: metrics [ \"slices\" ][ \"overall\" ][ metric ] = np . mean ( list ( itertools . chain . from_iterable ( [ [ metrics [ \"slices\" ][ \"class\" ][ slice_name ][ metric ]] * metrics [ \"slices\" ][ \"class\" ][ slice_name ][ \"num_samples\" ] for slice_name in metrics [ \"slices\" ][ \"class\" ] ] ) ) ) return metrics","title":"get_metrics()"},{"location":"tagifai/main/","text":"compute_features ( params_fp = PosixPath ( '/home/runner/work/follow/follow/config/params.json' )) Compute and save features for training. Parameters: Name Type Description Default params_fp Path Location of parameters (just using num_samples, num_epochs, etc.) to use for training. Defaults to config/params.json . PosixPath('/home/runner/work/follow/follow/config/params.json') Source code in tagifai/main.py @app . command () def compute_features ( params_fp : Path = Path ( config . CONFIG_DIR , \"params.json\" ), ) -> None : \"\"\"Compute and save features for training. Args: params_fp (Path, optional): Location of parameters (just using num_samples, num_epochs, etc.) to use for training. Defaults to `config/params.json`. \"\"\" # Parameters params = Namespace ( ** utils . load_dict ( filepath = params_fp )) # Compute features data . compute_features ( params = params ) logger . info ( \"\u2705 Computed features!\" ) delete_experiment ( experiment_name ) Delete an experiment with name experiment_name . Parameters: Name Type Description Default experiment_name str Name of the experiment. required Source code in tagifai/main.py def delete_experiment ( experiment_name : str ): \"\"\"Delete an experiment with name `experiment_name`. Args: experiment_name (str): Name of the experiment. \"\"\" client = mlflow . tracking . MlflowClient () experiment_id = client . get_experiment_by_name ( experiment_name ) . experiment_id client . delete_experiment ( experiment_id = experiment_id ) logger . info ( f \"\u2705 Deleted experiment { experiment_name } !\" ) get_historical_features () Retrieve historical features for training. Source code in tagifai/main.py @app . command () def get_historical_features (): \"\"\"Retrieve historical features for training.\"\"\" # Entities to pull data for (should dynamically read this from somewhere) project_ids = [ 1 , 2 , 3 ] now = datetime . now () timestamps = [ datetime ( now . year , now . month , now . day )] * len ( project_ids ) entity_df = pd . DataFrame . from_dict ({ \"id\" : project_ids , \"event_timestamp\" : timestamps }) # Get historical features store = FeatureStore ( repo_path = Path ( config . BASE_DIR , \"features\" )) training_df = store . get_historical_features ( entity_df = entity_df , feature_refs = [ \"project_details:text\" , \"project_details:tags\" ], ) . to_df () logger . info ( training_df . head ()) return training_df load_artifacts ( run_id , device = device ( type = 'cpu' )) Load artifacts for current model. Parameters: Name Type Description Default run_id str ID of the model run to load artifacts. Defaults to run ID in config.MODEL_DIR. required device device Device to run model on. Defaults to CPU. device(type='cpu') Returns: Type Description Dict Artifacts needed for inference. Source code in tagifai/main.py def load_artifacts ( run_id : str , device : torch . device = torch . device ( \"cpu\" )) -> Dict : \"\"\"Load artifacts for current model. Args: run_id (str): ID of the model run to load artifacts. Defaults to run ID in config.MODEL_DIR. device (torch.device): Device to run model on. Defaults to CPU. Returns: Artifacts needed for inference. \"\"\" # Load artifacts experiment_id = mlflow . get_run ( run_id = run_id ) . info . experiment_id artifacts_dir = Path ( config . MODEL_REGISTRY , experiment_id , run_id , \"artifacts\" ) params = Namespace ( ** utils . load_dict ( filepath = Path ( artifacts_dir , \"params.json\" ))) label_encoder = data . MultiLabelLabelEncoder . load ( fp = Path ( artifacts_dir , \"label_encoder.json\" )) tokenizer = data . Tokenizer . load ( fp = Path ( artifacts_dir , \"tokenizer.json\" )) model_state = torch . load ( Path ( artifacts_dir , \"model.pt\" ), map_location = device ) performance = utils . load_dict ( filepath = Path ( artifacts_dir , \"performance.json\" )) # Initialize model model = models . initialize_model ( params = params , vocab_size = len ( tokenizer ), num_classes = len ( label_encoder ) ) model . load_state_dict ( model_state ) return { \"params\" : params , \"label_encoder\" : label_encoder , \"tokenizer\" : tokenizer , \"model\" : model , \"performance\" : performance , } load_data () Load data from URLs and save to local drive. Source code in tagifai/main.py @app . command () def load_data (): \"\"\"Load data from URLs and save to local drive.\"\"\" # Download main data projects_url = ( \"https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/datasets/projects.json\" ) projects = utils . load_json_from_url ( url = projects_url ) projects_fp = Path ( config . DATA_DIR , \"projects.json\" ) utils . save_dict ( d = projects , filepath = projects_fp ) # Download auxiliary data tags_url = \"https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/datasets/tags.json\" tags = utils . load_json_from_url ( url = tags_url ) tags_fp = Path ( config . DATA_DIR , \"tags.json\" ) utils . save_dict ( d = tags , filepath = tags_fp ) logger . info ( \"\u2705 Loaded data!\" ) optimize ( params_fp = PosixPath ( '/home/runner/work/follow/follow/config/params.json' ), study_name = 'optimization' , num_trials = 100 ) Optimize a subset of hyperparameters towards an objective. This saves the best trial's parameters into config/params.json . Parameters: Name Type Description Default params_fp Path Location of parameters (just using num_samples, num_epochs, etc.) to use for training. Defaults to config/params.json . PosixPath('/home/runner/work/follow/follow/config/params.json') study_name Optional[str] Name of the study to save trial runs under. Defaults to optimization . 'optimization' num_trials int Number of trials to run. Defaults to 100. 100 Source code in tagifai/main.py @app . command () def optimize ( params_fp : Path = Path ( config . CONFIG_DIR , \"params.json\" ), study_name : Optional [ str ] = \"optimization\" , num_trials : int = 100 , ) -> None : \"\"\"Optimize a subset of hyperparameters towards an objective. This saves the best trial's parameters into `config/params.json`. Args: params_fp (Path, optional): Location of parameters (just using num_samples, num_epochs, etc.) to use for training. Defaults to `config/params.json`. study_name (str, optional): Name of the study to save trial runs under. Defaults to `optimization`. num_trials (int, optional): Number of trials to run. Defaults to 100. \"\"\" # Parameters params = Namespace ( ** utils . load_dict ( filepath = params_fp )) # Optimize pruner = optuna . pruners . MedianPruner ( n_startup_trials = 5 , n_warmup_steps = 5 ) study = optuna . create_study ( study_name = study_name , direction = \"maximize\" , pruner = pruner ) mlflow_callback = MLflowCallback ( tracking_uri = mlflow . get_tracking_uri (), metric_name = \"f1\" ) study . optimize ( lambda trial : train . objective ( params , trial ), n_trials = num_trials , callbacks = [ mlflow_callback ], ) # All trials trials_df = study . trials_dataframe () trials_df = trials_df . sort_values ([ \"value\" ], ascending = False ) # Best trial logger . info ( f \"Best value (f1): { study . best_trial . value } \" ) params = { ** params . __dict__ , ** study . best_trial . params } params [ \"threshold\" ] = study . best_trial . user_attrs [ \"threshold\" ] utils . save_dict ( params , params_fp , cls = NumpyEncoder ) logger . info ( json . dumps ( params , indent = 2 , cls = NumpyEncoder )) params ( run_id ) Configured parametes for a specific run ID. Source code in tagifai/main.py @app . command () def params ( run_id : str ) -> Dict : \"\"\"Configured parametes for a specific run ID.\"\"\" artifact_uri = mlflow . get_run ( run_id = run_id ) . info . artifact_uri . split ( \"file://\" )[ - 1 ] params = utils . load_dict ( filepath = Path ( artifact_uri , \"params.json\" )) logger . info ( json . dumps ( params , indent = 2 )) return params performance ( run_id ) Performance summary for a specific run ID. Source code in tagifai/main.py @app . command () def performance ( run_id : str ) -> Dict : \"\"\"Performance summary for a specific run ID.\"\"\" artifact_uri = mlflow . get_run ( run_id = run_id ) . info . artifact_uri . split ( \"file://\" )[ - 1 ] performance = utils . load_dict ( filepath = Path ( artifact_uri , \"performance.json\" )) logger . info ( json . dumps ( performance , indent = 2 )) return performance predict_tags ( text , run_id ) Predict tags for a give input text using a trained model. Warning Make sure that you have a trained model first! Parameters: Name Type Description Default text str Input text to predict tags for. required run_id str ID of the model run to load artifacts. required Exceptions: Type Description ValueError Run id doesn't exist in experiment. Returns: Type Description Dict Predicted tags for input text. Source code in tagifai/main.py @app . command () def predict_tags ( text : str , run_id : str ) -> Dict : \"\"\"Predict tags for a give input text using a trained model. Warning: Make sure that you have a trained model first! Args: text (str): Input text to predict tags for. run_id (str): ID of the model run to load artifacts. Raises: ValueError: Run id doesn't exist in experiment. Returns: Predicted tags for input text. \"\"\" # Predict artifacts = load_artifacts ( run_id = run_id ) prediction = predict . predict ( texts = [ text ], artifacts = artifacts ) logger . info ( json . dumps ( prediction , indent = 2 )) return prediction train_model ( params_fp = PosixPath ( '/home/runner/work/follow/follow/config/params.json' ), experiment_name = 'best' , run_name = 'model' , test_run = False ) Train a model using the specified parameters. Parameters: Name Type Description Default params_fp Path Parameters to use for training. Defaults to config/params.json . PosixPath('/home/runner/work/follow/follow/config/params.json') experiment_name Optional[str] Name of the experiment to save the run to. Defaults to best . 'best' run_name Optional[str] Name of the run. Defaults to model . 'model' test_run Optional[bool] Whether to run as a test or not. If True, artifacts will not be saved. Defaults to True. False Source code in tagifai/main.py @app . command () def train_model ( params_fp : Path = Path ( config . CONFIG_DIR , \"params.json\" ), experiment_name : Optional [ str ] = \"best\" , run_name : Optional [ str ] = \"model\" , test_run : Optional [ bool ] = False , ) -> None : \"\"\"Train a model using the specified parameters. Args: params_fp (Path, optional): Parameters to use for training. Defaults to `config/params.json`. experiment_name (str, optional): Name of the experiment to save the run to. Defaults to `best`. run_name (str, optional): Name of the run. Defaults to `model`. test_run (bool, optional): Whether to run as a test or not. If True, artifacts will not be saved. Defaults to True. \"\"\" # Parameters params = Namespace ( ** utils . load_dict ( filepath = params_fp )) # Start run mlflow . set_experiment ( experiment_name = experiment_name ) with mlflow . start_run ( run_name = run_name ): run_id = mlflow . active_run () . info . run_id logger . info ( f \"Run ID: { run_id } \" ) # Train artifacts = train . train ( params = params ) # Set tags tags = {} mlflow . set_tags ( tags ) # Log metrics performance = artifacts [ \"performance\" ] logger . info ( json . dumps ( performance [ \"overall\" ], indent = 2 )) metrics = { \"precision\" : performance [ \"overall\" ][ \"precision\" ], \"recall\" : performance [ \"overall\" ][ \"recall\" ], \"f1\" : performance [ \"overall\" ][ \"f1\" ], \"best_val_loss\" : artifacts [ \"loss\" ], } mlflow . log_metrics ( metrics ) # Log artifacts with tempfile . TemporaryDirectory () as dp : utils . save_dict ( vars ( artifacts [ \"params\" ]), Path ( dp , \"params.json\" ), cls = NumpyEncoder ) utils . save_dict ( performance , Path ( dp , \"performance.json\" )) artifacts [ \"label_encoder\" ] . save ( Path ( dp , \"label_encoder.json\" )) artifacts [ \"tokenizer\" ] . save ( Path ( dp , \"tokenizer.json\" )) torch . save ( artifacts [ \"model\" ] . state_dict (), Path ( dp , \"model.pt\" )) mlflow . log_artifacts ( dp ) mlflow . log_params ( vars ( artifacts [ \"params\" ])) # Save to config if not test_run : # pragma: no cover, testing shouldn't save files open ( Path ( config . CONFIG_DIR , \"run_id.txt\" ), \"w\" ) . write ( run_id ) utils . save_dict ( performance , Path ( config . CONFIG_DIR , \"performance.json\" ))","title":"Operations"},{"location":"tagifai/main/#tagifai.main.compute_features","text":"Compute and save features for training. Parameters: Name Type Description Default params_fp Path Location of parameters (just using num_samples, num_epochs, etc.) to use for training. Defaults to config/params.json . PosixPath('/home/runner/work/follow/follow/config/params.json') Source code in tagifai/main.py @app . command () def compute_features ( params_fp : Path = Path ( config . CONFIG_DIR , \"params.json\" ), ) -> None : \"\"\"Compute and save features for training. Args: params_fp (Path, optional): Location of parameters (just using num_samples, num_epochs, etc.) to use for training. Defaults to `config/params.json`. \"\"\" # Parameters params = Namespace ( ** utils . load_dict ( filepath = params_fp )) # Compute features data . compute_features ( params = params ) logger . info ( \"\u2705 Computed features!\" )","title":"compute_features()"},{"location":"tagifai/main/#tagifai.main.delete_experiment","text":"Delete an experiment with name experiment_name . Parameters: Name Type Description Default experiment_name str Name of the experiment. required Source code in tagifai/main.py def delete_experiment ( experiment_name : str ): \"\"\"Delete an experiment with name `experiment_name`. Args: experiment_name (str): Name of the experiment. \"\"\" client = mlflow . tracking . MlflowClient () experiment_id = client . get_experiment_by_name ( experiment_name ) . experiment_id client . delete_experiment ( experiment_id = experiment_id ) logger . info ( f \"\u2705 Deleted experiment { experiment_name } !\" )","title":"delete_experiment()"},{"location":"tagifai/main/#tagifai.main.get_historical_features","text":"Retrieve historical features for training. Source code in tagifai/main.py @app . command () def get_historical_features (): \"\"\"Retrieve historical features for training.\"\"\" # Entities to pull data for (should dynamically read this from somewhere) project_ids = [ 1 , 2 , 3 ] now = datetime . now () timestamps = [ datetime ( now . year , now . month , now . day )] * len ( project_ids ) entity_df = pd . DataFrame . from_dict ({ \"id\" : project_ids , \"event_timestamp\" : timestamps }) # Get historical features store = FeatureStore ( repo_path = Path ( config . BASE_DIR , \"features\" )) training_df = store . get_historical_features ( entity_df = entity_df , feature_refs = [ \"project_details:text\" , \"project_details:tags\" ], ) . to_df () logger . info ( training_df . head ()) return training_df","title":"get_historical_features()"},{"location":"tagifai/main/#tagifai.main.load_artifacts","text":"Load artifacts for current model. Parameters: Name Type Description Default run_id str ID of the model run to load artifacts. Defaults to run ID in config.MODEL_DIR. required device device Device to run model on. Defaults to CPU. device(type='cpu') Returns: Type Description Dict Artifacts needed for inference. Source code in tagifai/main.py def load_artifacts ( run_id : str , device : torch . device = torch . device ( \"cpu\" )) -> Dict : \"\"\"Load artifacts for current model. Args: run_id (str): ID of the model run to load artifacts. Defaults to run ID in config.MODEL_DIR. device (torch.device): Device to run model on. Defaults to CPU. Returns: Artifacts needed for inference. \"\"\" # Load artifacts experiment_id = mlflow . get_run ( run_id = run_id ) . info . experiment_id artifacts_dir = Path ( config . MODEL_REGISTRY , experiment_id , run_id , \"artifacts\" ) params = Namespace ( ** utils . load_dict ( filepath = Path ( artifacts_dir , \"params.json\" ))) label_encoder = data . MultiLabelLabelEncoder . load ( fp = Path ( artifacts_dir , \"label_encoder.json\" )) tokenizer = data . Tokenizer . load ( fp = Path ( artifacts_dir , \"tokenizer.json\" )) model_state = torch . load ( Path ( artifacts_dir , \"model.pt\" ), map_location = device ) performance = utils . load_dict ( filepath = Path ( artifacts_dir , \"performance.json\" )) # Initialize model model = models . initialize_model ( params = params , vocab_size = len ( tokenizer ), num_classes = len ( label_encoder ) ) model . load_state_dict ( model_state ) return { \"params\" : params , \"label_encoder\" : label_encoder , \"tokenizer\" : tokenizer , \"model\" : model , \"performance\" : performance , }","title":"load_artifacts()"},{"location":"tagifai/main/#tagifai.main.load_data","text":"Load data from URLs and save to local drive. Source code in tagifai/main.py @app . command () def load_data (): \"\"\"Load data from URLs and save to local drive.\"\"\" # Download main data projects_url = ( \"https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/datasets/projects.json\" ) projects = utils . load_json_from_url ( url = projects_url ) projects_fp = Path ( config . DATA_DIR , \"projects.json\" ) utils . save_dict ( d = projects , filepath = projects_fp ) # Download auxiliary data tags_url = \"https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/datasets/tags.json\" tags = utils . load_json_from_url ( url = tags_url ) tags_fp = Path ( config . DATA_DIR , \"tags.json\" ) utils . save_dict ( d = tags , filepath = tags_fp ) logger . info ( \"\u2705 Loaded data!\" )","title":"load_data()"},{"location":"tagifai/main/#tagifai.main.optimize","text":"Optimize a subset of hyperparameters towards an objective. This saves the best trial's parameters into config/params.json . Parameters: Name Type Description Default params_fp Path Location of parameters (just using num_samples, num_epochs, etc.) to use for training. Defaults to config/params.json . PosixPath('/home/runner/work/follow/follow/config/params.json') study_name Optional[str] Name of the study to save trial runs under. Defaults to optimization . 'optimization' num_trials int Number of trials to run. Defaults to 100. 100 Source code in tagifai/main.py @app . command () def optimize ( params_fp : Path = Path ( config . CONFIG_DIR , \"params.json\" ), study_name : Optional [ str ] = \"optimization\" , num_trials : int = 100 , ) -> None : \"\"\"Optimize a subset of hyperparameters towards an objective. This saves the best trial's parameters into `config/params.json`. Args: params_fp (Path, optional): Location of parameters (just using num_samples, num_epochs, etc.) to use for training. Defaults to `config/params.json`. study_name (str, optional): Name of the study to save trial runs under. Defaults to `optimization`. num_trials (int, optional): Number of trials to run. Defaults to 100. \"\"\" # Parameters params = Namespace ( ** utils . load_dict ( filepath = params_fp )) # Optimize pruner = optuna . pruners . MedianPruner ( n_startup_trials = 5 , n_warmup_steps = 5 ) study = optuna . create_study ( study_name = study_name , direction = \"maximize\" , pruner = pruner ) mlflow_callback = MLflowCallback ( tracking_uri = mlflow . get_tracking_uri (), metric_name = \"f1\" ) study . optimize ( lambda trial : train . objective ( params , trial ), n_trials = num_trials , callbacks = [ mlflow_callback ], ) # All trials trials_df = study . trials_dataframe () trials_df = trials_df . sort_values ([ \"value\" ], ascending = False ) # Best trial logger . info ( f \"Best value (f1): { study . best_trial . value } \" ) params = { ** params . __dict__ , ** study . best_trial . params } params [ \"threshold\" ] = study . best_trial . user_attrs [ \"threshold\" ] utils . save_dict ( params , params_fp , cls = NumpyEncoder ) logger . info ( json . dumps ( params , indent = 2 , cls = NumpyEncoder ))","title":"optimize()"},{"location":"tagifai/main/#tagifai.main.params","text":"Configured parametes for a specific run ID. Source code in tagifai/main.py @app . command () def params ( run_id : str ) -> Dict : \"\"\"Configured parametes for a specific run ID.\"\"\" artifact_uri = mlflow . get_run ( run_id = run_id ) . info . artifact_uri . split ( \"file://\" )[ - 1 ] params = utils . load_dict ( filepath = Path ( artifact_uri , \"params.json\" )) logger . info ( json . dumps ( params , indent = 2 )) return params","title":"params()"},{"location":"tagifai/main/#tagifai.main.performance","text":"Performance summary for a specific run ID. Source code in tagifai/main.py @app . command () def performance ( run_id : str ) -> Dict : \"\"\"Performance summary for a specific run ID.\"\"\" artifact_uri = mlflow . get_run ( run_id = run_id ) . info . artifact_uri . split ( \"file://\" )[ - 1 ] performance = utils . load_dict ( filepath = Path ( artifact_uri , \"performance.json\" )) logger . info ( json . dumps ( performance , indent = 2 )) return performance","title":"performance()"},{"location":"tagifai/main/#tagifai.main.predict_tags","text":"Predict tags for a give input text using a trained model. Warning Make sure that you have a trained model first! Parameters: Name Type Description Default text str Input text to predict tags for. required run_id str ID of the model run to load artifacts. required Exceptions: Type Description ValueError Run id doesn't exist in experiment. Returns: Type Description Dict Predicted tags for input text. Source code in tagifai/main.py @app . command () def predict_tags ( text : str , run_id : str ) -> Dict : \"\"\"Predict tags for a give input text using a trained model. Warning: Make sure that you have a trained model first! Args: text (str): Input text to predict tags for. run_id (str): ID of the model run to load artifacts. Raises: ValueError: Run id doesn't exist in experiment. Returns: Predicted tags for input text. \"\"\" # Predict artifacts = load_artifacts ( run_id = run_id ) prediction = predict . predict ( texts = [ text ], artifacts = artifacts ) logger . info ( json . dumps ( prediction , indent = 2 )) return prediction","title":"predict_tags()"},{"location":"tagifai/main/#tagifai.main.train_model","text":"Train a model using the specified parameters. Parameters: Name Type Description Default params_fp Path Parameters to use for training. Defaults to config/params.json . PosixPath('/home/runner/work/follow/follow/config/params.json') experiment_name Optional[str] Name of the experiment to save the run to. Defaults to best . 'best' run_name Optional[str] Name of the run. Defaults to model . 'model' test_run Optional[bool] Whether to run as a test or not. If True, artifacts will not be saved. Defaults to True. False Source code in tagifai/main.py @app . command () def train_model ( params_fp : Path = Path ( config . CONFIG_DIR , \"params.json\" ), experiment_name : Optional [ str ] = \"best\" , run_name : Optional [ str ] = \"model\" , test_run : Optional [ bool ] = False , ) -> None : \"\"\"Train a model using the specified parameters. Args: params_fp (Path, optional): Parameters to use for training. Defaults to `config/params.json`. experiment_name (str, optional): Name of the experiment to save the run to. Defaults to `best`. run_name (str, optional): Name of the run. Defaults to `model`. test_run (bool, optional): Whether to run as a test or not. If True, artifacts will not be saved. Defaults to True. \"\"\" # Parameters params = Namespace ( ** utils . load_dict ( filepath = params_fp )) # Start run mlflow . set_experiment ( experiment_name = experiment_name ) with mlflow . start_run ( run_name = run_name ): run_id = mlflow . active_run () . info . run_id logger . info ( f \"Run ID: { run_id } \" ) # Train artifacts = train . train ( params = params ) # Set tags tags = {} mlflow . set_tags ( tags ) # Log metrics performance = artifacts [ \"performance\" ] logger . info ( json . dumps ( performance [ \"overall\" ], indent = 2 )) metrics = { \"precision\" : performance [ \"overall\" ][ \"precision\" ], \"recall\" : performance [ \"overall\" ][ \"recall\" ], \"f1\" : performance [ \"overall\" ][ \"f1\" ], \"best_val_loss\" : artifacts [ \"loss\" ], } mlflow . log_metrics ( metrics ) # Log artifacts with tempfile . TemporaryDirectory () as dp : utils . save_dict ( vars ( artifacts [ \"params\" ]), Path ( dp , \"params.json\" ), cls = NumpyEncoder ) utils . save_dict ( performance , Path ( dp , \"performance.json\" )) artifacts [ \"label_encoder\" ] . save ( Path ( dp , \"label_encoder.json\" )) artifacts [ \"tokenizer\" ] . save ( Path ( dp , \"tokenizer.json\" )) torch . save ( artifacts [ \"model\" ] . state_dict (), Path ( dp , \"model.pt\" )) mlflow . log_artifacts ( dp ) mlflow . log_params ( vars ( artifacts [ \"params\" ])) # Save to config if not test_run : # pragma: no cover, testing shouldn't save files open ( Path ( config . CONFIG_DIR , \"run_id.txt\" ), \"w\" ) . write ( run_id ) utils . save_dict ( performance , Path ( config . CONFIG_DIR , \"performance.json\" ))","title":"train_model()"},{"location":"tagifai/models/","text":"CNN __init__ ( self , embedding_dim , vocab_size , num_filters , filter_sizes , hidden_dim , dropout_p , num_classes , padding_idx = 0 ) special A convolutional neural network {:target=\"_blank\"} architecture created for natural language processing tasks where filters convolve across the given text inputs. Usage: # Initialize model filter_sizes = list ( range ( 1 , int ( params . max_filter_size ) + 1 )) model = models . CNN ( embedding_dim = int ( params . embedding_dim ), vocab_size = int ( vocab_size ), num_filters = int ( params . num_filters ), filter_sizes = filter_sizes , hidden_dim = int ( params . hidden_dim ), dropout_p = float ( params . dropout_p ), num_classes = int ( num_classes ), ) model = model . to ( device ) Parameters: Name Type Description Default embedding_dim int Embedding dimension for tokens. required vocab_size int Number of unique tokens in vocabulary. required num_filters int Number of filters per filter size. required filter_sizes list List of filter sizes for the CNN. required hidden_dim int Hidden dimension for fully-connected (FC) layers. required dropout_p float Dropout proportion for FC layers. required num_classes int Number of unique classes to classify into. required padding_idx int Index representing the <PAD> token. Defaults to 0. 0 Source code in tagifai/models.py def __init__ ( self , embedding_dim : int , vocab_size : int , num_filters : int , filter_sizes : list , hidden_dim : int , dropout_p : float , num_classes : int , padding_idx : int = 0 , ) -> None : \"\"\"A [convolutional neural network](https://madewithml.com/courses/foundations/convolutional-neural-networks/){:target=\"_blank\"} architecture created for natural language processing tasks where filters convolve across the given text inputs. ![text CNN](https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/images/foundations/embeddings/model.png) Usage: ```python # Initialize model filter_sizes = list(range(1, int(params.max_filter_size) + 1)) model = models.CNN( embedding_dim=int(params.embedding_dim), vocab_size=int(vocab_size), num_filters=int(params.num_filters), filter_sizes=filter_sizes, hidden_dim=int(params.hidden_dim), dropout_p=float(params.dropout_p), num_classes=int(num_classes), ) model = model.to(device) ``` Args: embedding_dim (int): Embedding dimension for tokens. vocab_size (int): Number of unique tokens in vocabulary. num_filters (int): Number of filters per filter size. filter_sizes (list): List of filter sizes for the CNN. hidden_dim (int): Hidden dimension for fully-connected (FC) layers. dropout_p (float): Dropout proportion for FC layers. num_classes (int): Number of unique classes to classify into. padding_idx (int, optional): Index representing the `<PAD>` token. Defaults to 0. \"\"\" super () . __init__ () # Initialize embeddings self . embeddings = nn . Embedding ( embedding_dim = embedding_dim , num_embeddings = vocab_size , padding_idx = padding_idx , ) # Conv weights self . filter_sizes = filter_sizes self . conv = nn . ModuleList ( [ nn . Conv1d ( in_channels = embedding_dim , out_channels = num_filters , kernel_size = f , ) for f in filter_sizes ] ) # FC weights self . dropout = nn . Dropout ( dropout_p ) self . fc1 = nn . Linear ( num_filters * len ( filter_sizes ), hidden_dim ) self . fc2 = nn . Linear ( hidden_dim , num_classes ) forward ( self , inputs , channel_first = False ) Forward pass. Parameters: Name Type Description Default inputs List List of inputs (by feature). required channel_first bool Channel dimension is first in inputs. Defaults to False. False Returns: Type Description Tensor Outputs from the model. Source code in tagifai/models.py def forward ( self , inputs : List , channel_first : bool = False ) -> torch . Tensor : \"\"\"Forward pass. Args: inputs (List): List of inputs (by feature). channel_first (bool, optional): Channel dimension is first in inputs. Defaults to False. Returns: Outputs from the model. \"\"\" # Embed ( x_in ,) = inputs x_in = self . embeddings ( x_in ) if not channel_first : x_in = x_in . transpose ( 1 , 2 ) # (N, channels, sequence length) z = [] max_seq_len = x_in . shape [ 2 ] for i , f in enumerate ( self . filter_sizes ): # `SAME` padding padding_left = int ( ( self . conv [ i ] . stride [ 0 ] * ( max_seq_len - 1 ) - max_seq_len + self . filter_sizes [ i ]) / 2 ) padding_right = int ( math . ceil ( ( self . conv [ i ] . stride [ 0 ] * ( max_seq_len - 1 ) - max_seq_len + self . filter_sizes [ i ] ) / 2 ) ) # Conv _z = self . conv [ i ]( F . pad ( x_in , ( padding_left , padding_right ))) # Pool _z = F . max_pool1d ( _z , _z . size ( 2 )) . squeeze ( 2 ) z . append ( _z ) # Concat outputs z = torch . cat ( z , 1 ) # FC z = self . fc1 ( z ) z = self . dropout ( z ) z = self . fc2 ( z ) return z initialize_model ( params , vocab_size , num_classes , device = device ( type = 'cpu' )) Initialize a model using parameters (converted to appropriate data types). Parameters: Name Type Description Default params Namespace Parameters for data processing and training. required vocab_size int Size of the vocabulary. required num_classes int Number on unique classes. required device device Device to run model on. Defaults to CPU. device(type='cpu') Returns: Type Description Module Initialize torch model instance. Source code in tagifai/models.py def initialize_model ( params : Namespace , vocab_size : int , num_classes : int , device : torch . device = torch . device ( \"cpu\" ), ) -> nn . Module : \"\"\"Initialize a model using parameters (converted to appropriate data types). Args: params (Namespace): Parameters for data processing and training. vocab_size (int): Size of the vocabulary. num_classes (int): Number on unique classes. device (torch.device): Device to run model on. Defaults to CPU. Returns: Initialize torch model instance. \"\"\" # Initialize model filter_sizes = list ( range ( 1 , int ( params . max_filter_size ) + 1 )) model = CNN ( embedding_dim = int ( params . embedding_dim ), vocab_size = int ( vocab_size ), num_filters = int ( params . num_filters ), filter_sizes = filter_sizes , hidden_dim = int ( params . hidden_dim ), dropout_p = float ( params . dropout_p ), num_classes = int ( num_classes ), ) model = model . to ( device ) return model","title":"Models"},{"location":"tagifai/models/#tagifai.models.CNN","text":"","title":"CNN"},{"location":"tagifai/models/#tagifai.models.CNN.__init__","text":"A convolutional neural network {:target=\"_blank\"} architecture created for natural language processing tasks where filters convolve across the given text inputs. Usage: # Initialize model filter_sizes = list ( range ( 1 , int ( params . max_filter_size ) + 1 )) model = models . CNN ( embedding_dim = int ( params . embedding_dim ), vocab_size = int ( vocab_size ), num_filters = int ( params . num_filters ), filter_sizes = filter_sizes , hidden_dim = int ( params . hidden_dim ), dropout_p = float ( params . dropout_p ), num_classes = int ( num_classes ), ) model = model . to ( device ) Parameters: Name Type Description Default embedding_dim int Embedding dimension for tokens. required vocab_size int Number of unique tokens in vocabulary. required num_filters int Number of filters per filter size. required filter_sizes list List of filter sizes for the CNN. required hidden_dim int Hidden dimension for fully-connected (FC) layers. required dropout_p float Dropout proportion for FC layers. required num_classes int Number of unique classes to classify into. required padding_idx int Index representing the <PAD> token. Defaults to 0. 0 Source code in tagifai/models.py def __init__ ( self , embedding_dim : int , vocab_size : int , num_filters : int , filter_sizes : list , hidden_dim : int , dropout_p : float , num_classes : int , padding_idx : int = 0 , ) -> None : \"\"\"A [convolutional neural network](https://madewithml.com/courses/foundations/convolutional-neural-networks/){:target=\"_blank\"} architecture created for natural language processing tasks where filters convolve across the given text inputs. ![text CNN](https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/images/foundations/embeddings/model.png) Usage: ```python # Initialize model filter_sizes = list(range(1, int(params.max_filter_size) + 1)) model = models.CNN( embedding_dim=int(params.embedding_dim), vocab_size=int(vocab_size), num_filters=int(params.num_filters), filter_sizes=filter_sizes, hidden_dim=int(params.hidden_dim), dropout_p=float(params.dropout_p), num_classes=int(num_classes), ) model = model.to(device) ``` Args: embedding_dim (int): Embedding dimension for tokens. vocab_size (int): Number of unique tokens in vocabulary. num_filters (int): Number of filters per filter size. filter_sizes (list): List of filter sizes for the CNN. hidden_dim (int): Hidden dimension for fully-connected (FC) layers. dropout_p (float): Dropout proportion for FC layers. num_classes (int): Number of unique classes to classify into. padding_idx (int, optional): Index representing the `<PAD>` token. Defaults to 0. \"\"\" super () . __init__ () # Initialize embeddings self . embeddings = nn . Embedding ( embedding_dim = embedding_dim , num_embeddings = vocab_size , padding_idx = padding_idx , ) # Conv weights self . filter_sizes = filter_sizes self . conv = nn . ModuleList ( [ nn . Conv1d ( in_channels = embedding_dim , out_channels = num_filters , kernel_size = f , ) for f in filter_sizes ] ) # FC weights self . dropout = nn . Dropout ( dropout_p ) self . fc1 = nn . Linear ( num_filters * len ( filter_sizes ), hidden_dim ) self . fc2 = nn . Linear ( hidden_dim , num_classes )","title":"__init__()"},{"location":"tagifai/models/#tagifai.models.CNN.forward","text":"Forward pass. Parameters: Name Type Description Default inputs List List of inputs (by feature). required channel_first bool Channel dimension is first in inputs. Defaults to False. False Returns: Type Description Tensor Outputs from the model. Source code in tagifai/models.py def forward ( self , inputs : List , channel_first : bool = False ) -> torch . Tensor : \"\"\"Forward pass. Args: inputs (List): List of inputs (by feature). channel_first (bool, optional): Channel dimension is first in inputs. Defaults to False. Returns: Outputs from the model. \"\"\" # Embed ( x_in ,) = inputs x_in = self . embeddings ( x_in ) if not channel_first : x_in = x_in . transpose ( 1 , 2 ) # (N, channels, sequence length) z = [] max_seq_len = x_in . shape [ 2 ] for i , f in enumerate ( self . filter_sizes ): # `SAME` padding padding_left = int ( ( self . conv [ i ] . stride [ 0 ] * ( max_seq_len - 1 ) - max_seq_len + self . filter_sizes [ i ]) / 2 ) padding_right = int ( math . ceil ( ( self . conv [ i ] . stride [ 0 ] * ( max_seq_len - 1 ) - max_seq_len + self . filter_sizes [ i ] ) / 2 ) ) # Conv _z = self . conv [ i ]( F . pad ( x_in , ( padding_left , padding_right ))) # Pool _z = F . max_pool1d ( _z , _z . size ( 2 )) . squeeze ( 2 ) z . append ( _z ) # Concat outputs z = torch . cat ( z , 1 ) # FC z = self . fc1 ( z ) z = self . dropout ( z ) z = self . fc2 ( z ) return z","title":"forward()"},{"location":"tagifai/models/#tagifai.models.initialize_model","text":"Initialize a model using parameters (converted to appropriate data types). Parameters: Name Type Description Default params Namespace Parameters for data processing and training. required vocab_size int Size of the vocabulary. required num_classes int Number on unique classes. required device device Device to run model on. Defaults to CPU. device(type='cpu') Returns: Type Description Module Initialize torch model instance. Source code in tagifai/models.py def initialize_model ( params : Namespace , vocab_size : int , num_classes : int , device : torch . device = torch . device ( \"cpu\" ), ) -> nn . Module : \"\"\"Initialize a model using parameters (converted to appropriate data types). Args: params (Namespace): Parameters for data processing and training. vocab_size (int): Size of the vocabulary. num_classes (int): Number on unique classes. device (torch.device): Device to run model on. Defaults to CPU. Returns: Initialize torch model instance. \"\"\" # Initialize model filter_sizes = list ( range ( 1 , int ( params . max_filter_size ) + 1 )) model = CNN ( embedding_dim = int ( params . embedding_dim ), vocab_size = int ( vocab_size ), num_filters = int ( params . num_filters ), filter_sizes = filter_sizes , hidden_dim = int ( params . hidden_dim ), dropout_p = float ( params . dropout_p ), num_classes = int ( num_classes ), ) model = model . to ( device ) return model","title":"initialize_model()"},{"location":"tagifai/predict/","text":"predict ( texts , artifacts , device = device ( type = 'cpu' )) Predict tags for an input text using the best model from the best experiment. Usage: texts = [ \"Transfer learning with BERT.\" ] artifacts = load_artifacts ( run_id = \"264ac530b78c42608e5dea1086bc2c73\" ) predict ( texts = texts , artifacts = artifacts ) [ { \"input_text\": \"Transfer learning with BERT.\", \"preprocessed_text\": \"transfer learning bert\", \"predicted_tags\": [ \"attention\", \"language-modeling\", \"natural-language-processing\", \"transfer-learning\", \"transformers\" ] } ] Note The input parameter texts can hold multiple input texts and so the resulting prediction dictionary will have len(texts) items. Parameters: Name Type Description Default texts List List of input texts to predict tags for. required artifacts Dict Artifacts needed for inference. required device device Device to run model on. Defaults to CPU. device(type='cpu') Returns: Type Description Dict Predicted tags for each of the input texts. Source code in tagifai/predict.py def predict ( texts : List , artifacts : Dict , device : torch . device = torch . device ( \"cpu\" )) -> Dict : \"\"\"Predict tags for an input text using the best model from the `best` experiment. Usage: ```python texts = [\"Transfer learning with BERT.\"] artifacts = load_artifacts(run_id=\"264ac530b78c42608e5dea1086bc2c73\") predict(texts=texts, artifacts=artifacts) ``` <pre> [ { \"input_text\": \"Transfer learning with BERT.\", \"preprocessed_text\": \"transfer learning bert\", \"predicted_tags\": [ \"attention\", \"language-modeling\", \"natural-language-processing\", \"transfer-learning\", \"transformers\" ] } ] </pre> Note: The input parameter `texts` can hold multiple input texts and so the resulting prediction dictionary will have `len(texts)` items. Args: texts (List): List of input texts to predict tags for. artifacts (Dict): Artifacts needed for inference. device (torch.device): Device to run model on. Defaults to CPU. Returns: Predicted tags for each of the input texts. \"\"\" # Retrieve artifacts params = artifacts [ \"params\" ] label_encoder = artifacts [ \"label_encoder\" ] tokenizer = artifacts [ \"tokenizer\" ] model = artifacts [ \"model\" ] # Prepare data preprocessed_texts = [ data . preprocess ( text , lower = bool ( strtobool ( str ( params . lower ))), # params.lower could be str/bool stem = bool ( strtobool ( str ( params . stem ))), ) for text in texts ] X = np . array ( tokenizer . texts_to_sequences ( preprocessed_texts ), dtype = \"object\" ) y_filler = np . zeros (( len ( X ), len ( label_encoder ))) dataset = data . CNNTextDataset ( X = X , y = y_filler , max_filter_size = int ( params . max_filter_size )) dataloader = dataset . create_dataloader ( batch_size = int ( params . batch_size )) # Get predictions trainer = train . Trainer ( model = model , device = device ) _ , y_prob = trainer . predict_step ( dataloader ) y_pred = [ np . where ( prob >= float ( params . threshold ), 1 , 0 ) for prob in y_prob ] tags = label_encoder . decode ( y_pred ) predictions = [ { \"input_text\" : texts [ i ], \"preprocessed_text\" : preprocessed_texts [ i ], \"predicted_tags\" : tags [ i ], } for i in range ( len ( tags )) ] return predictions","title":"Inference"},{"location":"tagifai/predict/#tagifai.predict.predict","text":"Predict tags for an input text using the best model from the best experiment. Usage: texts = [ \"Transfer learning with BERT.\" ] artifacts = load_artifacts ( run_id = \"264ac530b78c42608e5dea1086bc2c73\" ) predict ( texts = texts , artifacts = artifacts ) [ { \"input_text\": \"Transfer learning with BERT.\", \"preprocessed_text\": \"transfer learning bert\", \"predicted_tags\": [ \"attention\", \"language-modeling\", \"natural-language-processing\", \"transfer-learning\", \"transformers\" ] } ] Note The input parameter texts can hold multiple input texts and so the resulting prediction dictionary will have len(texts) items. Parameters: Name Type Description Default texts List List of input texts to predict tags for. required artifacts Dict Artifacts needed for inference. required device device Device to run model on. Defaults to CPU. device(type='cpu') Returns: Type Description Dict Predicted tags for each of the input texts. Source code in tagifai/predict.py def predict ( texts : List , artifacts : Dict , device : torch . device = torch . device ( \"cpu\" )) -> Dict : \"\"\"Predict tags for an input text using the best model from the `best` experiment. Usage: ```python texts = [\"Transfer learning with BERT.\"] artifacts = load_artifacts(run_id=\"264ac530b78c42608e5dea1086bc2c73\") predict(texts=texts, artifacts=artifacts) ``` <pre> [ { \"input_text\": \"Transfer learning with BERT.\", \"preprocessed_text\": \"transfer learning bert\", \"predicted_tags\": [ \"attention\", \"language-modeling\", \"natural-language-processing\", \"transfer-learning\", \"transformers\" ] } ] </pre> Note: The input parameter `texts` can hold multiple input texts and so the resulting prediction dictionary will have `len(texts)` items. Args: texts (List): List of input texts to predict tags for. artifacts (Dict): Artifacts needed for inference. device (torch.device): Device to run model on. Defaults to CPU. Returns: Predicted tags for each of the input texts. \"\"\" # Retrieve artifacts params = artifacts [ \"params\" ] label_encoder = artifacts [ \"label_encoder\" ] tokenizer = artifacts [ \"tokenizer\" ] model = artifacts [ \"model\" ] # Prepare data preprocessed_texts = [ data . preprocess ( text , lower = bool ( strtobool ( str ( params . lower ))), # params.lower could be str/bool stem = bool ( strtobool ( str ( params . stem ))), ) for text in texts ] X = np . array ( tokenizer . texts_to_sequences ( preprocessed_texts ), dtype = \"object\" ) y_filler = np . zeros (( len ( X ), len ( label_encoder ))) dataset = data . CNNTextDataset ( X = X , y = y_filler , max_filter_size = int ( params . max_filter_size )) dataloader = dataset . create_dataloader ( batch_size = int ( params . batch_size )) # Get predictions trainer = train . Trainer ( model = model , device = device ) _ , y_prob = trainer . predict_step ( dataloader ) y_pred = [ np . where ( prob >= float ( params . threshold ), 1 , 0 ) for prob in y_prob ] tags = label_encoder . decode ( y_pred ) predictions = [ { \"input_text\" : texts [ i ], \"preprocessed_text\" : preprocessed_texts [ i ], \"predicted_tags\" : tags [ i ], } for i in range ( len ( tags )) ] return predictions","title":"predict()"},{"location":"tagifai/train/","text":"Trainer Object used to facilitate training. eval_step ( self , dataloader ) Evaluation (val / test) step. Parameters: Name Type Description Default dataloader DataLoader Torch dataloader to load batches from. required Source code in tagifai/train.py def eval_step ( self , dataloader : torch . utils . data . DataLoader ): \"\"\"Evaluation (val / test) step. Args: dataloader (torch.utils.data.DataLoader): Torch dataloader to load batches from. \"\"\" # Set model to eval mode self . model . eval () loss = 0.0 y_trues , y_probs = [], [] # Iterate over val batches with torch . inference_mode (): for i , batch in enumerate ( dataloader ): # Step batch = [ item . to ( self . device ) for item in batch ] # Set device inputs , y_true = batch [: - 1 ], batch [ - 1 ] z = self . model ( inputs ) # Forward pass J = self . loss_fn ( z , y_true ) . item () # Cumulative Metrics loss += ( J - loss ) / ( i + 1 ) # Store outputs y_prob = torch . sigmoid ( z ) . cpu () . numpy () y_probs . extend ( y_prob ) y_trues . extend ( y_true . cpu () . numpy ()) return loss , np . vstack ( y_trues ), np . vstack ( y_probs ) predict_step ( self , dataloader ) Prediction (inference) step. Note Loss is not calculated for this loop. Parameters: Name Type Description Default dataloader DataLoader Torch dataloader to load batches from. required Source code in tagifai/train.py def predict_step ( self , dataloader : torch . utils . data . DataLoader ): \"\"\"Prediction (inference) step. Note: Loss is not calculated for this loop. Args: dataloader (torch.utils.data.DataLoader): Torch dataloader to load batches from. \"\"\" # Set model to eval mode self . model . eval () y_trues , y_probs = [], [] # Iterate over batches with torch . inference_mode (): for i , batch in enumerate ( dataloader ): # Forward pass w/ inputs batch = [ item . to ( self . device ) for item in batch ] # Set device inputs , y_true = batch [: - 1 ], batch [ - 1 ] z = self . model ( inputs ) # Store outputs y_prob = torch . sigmoid ( z ) . cpu () . numpy () y_probs . extend ( y_prob ) y_trues . extend ( y_true . cpu () . numpy ()) return np . vstack ( y_trues ), np . vstack ( y_probs ) train ( self , num_epochs , patience , train_dataloader , val_dataloader ) Training loop. Parameters: Name Type Description Default num_epochs int Maximum number of epochs to train for (can stop earlier based on performance). required patience int Number of acceptable epochs for continuous degrading performance. required train_dataloader DataLoader Dataloader object with training data split. required val_dataloader DataLoader Dataloader object with validation data split. required Exceptions: Type Description optuna.TrialPruned Early stopping of the optimization trial if poor performance. Returns: Type Description Tuple The best validation loss and the trained model from that point. Source code in tagifai/train.py def train ( self , num_epochs : int , patience : int , train_dataloader : torch . utils . data . DataLoader , val_dataloader : torch . utils . data . DataLoader , ) -> Tuple : \"\"\"Training loop. Args: num_epochs (int): Maximum number of epochs to train for (can stop earlier based on performance). patience (int): Number of acceptable epochs for continuous degrading performance. train_dataloader (torch.utils.data.DataLoader): Dataloader object with training data split. val_dataloader (torch.utils.data.DataLoader): Dataloader object with validation data split. Raises: optuna.TrialPruned: Early stopping of the optimization trial if poor performance. Returns: The best validation loss and the trained model from that point. \"\"\" best_val_loss = np . inf best_model = None _patience = patience for epoch in range ( num_epochs ): # Steps train_loss = self . train_step ( dataloader = train_dataloader ) val_loss , _ , _ = self . eval_step ( dataloader = val_dataloader ) self . scheduler . step ( val_loss ) # Pruning based on the intermediate value if self . trial : self . trial . report ( val_loss , epoch ) if self . trial . should_prune (): # pragma: no cover, optuna pruning logger . info ( \"Unpromising trial pruned!\" ) raise optuna . TrialPruned () # Early stopping if val_loss < best_val_loss : best_val_loss = val_loss best_model = self . model _patience = patience # reset _patience else : # pragma: no cover, simple subtraction _patience -= 1 if not _patience : # pragma: no cover, simple break logger . info ( \"Stopping early!\" ) break # Logging logger . info ( f \"Epoch: { epoch + 1 } | \" f \"train_loss: { train_loss : .5f } , \" f \"val_loss: { val_loss : .5f } , \" f \"lr: { self . optimizer . param_groups [ 0 ][ 'lr' ] : .2E } , \" f \"_patience: { _patience } \" ) return best_val_loss , best_model train_step ( self , dataloader ) Train step. Parameters: Name Type Description Default dataloader DataLoader Torch dataloader to load batches from. required Source code in tagifai/train.py def train_step ( self , dataloader : torch . utils . data . DataLoader ): \"\"\"Train step. Args: dataloader (torch.utils.data.DataLoader): Torch dataloader to load batches from. \"\"\" # Set model to train mode self . model . train () loss = 0.0 # Iterate over train batches for i , batch in enumerate ( dataloader ): # Step batch = [ item . to ( self . device ) for item in batch ] # Set device inputs , targets = batch [: - 1 ], batch [ - 1 ] self . optimizer . zero_grad () # Reset gradients z = self . model ( inputs ) # Forward pass J = self . loss_fn ( z , targets ) # Define loss J . backward () # Backward pass self . optimizer . step () # Update weights # Cumulative Metrics loss += ( J . detach () . item () - loss ) / ( i + 1 ) return loss find_best_threshold ( y_true , y_prob ) Determine the best threshold for maximum f1 score. Usage: # Find best threshold _ , y_true , y_prob = trainer . eval_step ( dataloader = train_dataloader ) params . threshold = find_best_threshold ( y_true = y_true , y_prob = y_prob ) Parameters: Name Type Description Default y_true ndarray True labels. required y_prob ndarray Probability distribution for predicted labels. required Returns: Type Description float Best threshold for maximum f1 score. Source code in tagifai/train.py def find_best_threshold ( y_true : np . ndarray , y_prob : np . ndarray ) -> float : \"\"\"Determine the best threshold for maximum f1 score. Usage: ```python # Find best threshold _, y_true, y_prob = trainer.eval_step(dataloader=train_dataloader) params.threshold = find_best_threshold(y_true=y_true, y_prob=y_prob) ``` Args: y_true (np.ndarray): True labels. y_prob (np.ndarray): Probability distribution for predicted labels. Returns: Best threshold for maximum f1 score. \"\"\" precisions , recalls , thresholds = precision_recall_curve ( y_true . ravel (), y_prob . ravel ()) f1s = ( 2 * precisions * recalls ) / ( precisions + recalls ) return thresholds [ np . argmax ( f1s )] objective ( params , trial ) Objective function for optimization trials. Parameters: Name Type Description Default params Namespace Input parameters for each trial (see config/params.json ). required trial Trial Optuna optimization trial. required Returns: Type Description float F1 score from evaluating the trained model on the test data split. Source code in tagifai/train.py def objective ( params : Namespace , trial : optuna . trial . _trial . Trial ) -> float : \"\"\"Objective function for optimization trials. Args: params (Namespace): Input parameters for each trial (see `config/params.json`). trial (optuna.trial._trial.Trial): Optuna optimization trial. Returns: F1 score from evaluating the trained model on the test data split. \"\"\" # Paramters (to tune) params . embedding_dim = trial . suggest_int ( \"embedding_dim\" , 128 , 512 ) params . num_filters = trial . suggest_int ( \"num_filters\" , 128 , 512 ) params . hidden_dim = trial . suggest_int ( \"hidden_dim\" , 128 , 512 ) params . dropout_p = trial . suggest_uniform ( \"dropout_p\" , 0.3 , 0.8 ) params . lr = trial . suggest_loguniform ( \"lr\" , 5e-5 , 5e-4 ) # Train (can move some of these outside for efficiency) logger . info ( f \" \\n Trial { trial . number } :\" ) logger . info ( json . dumps ( trial . params , indent = 2 )) artifacts = train ( params = params , trial = trial ) # Set additional attributes params = artifacts [ \"params\" ] performance = artifacts [ \"performance\" ] logger . info ( json . dumps ( performance [ \"overall\" ], indent = 2 )) trial . set_user_attr ( \"threshold\" , params . threshold ) trial . set_user_attr ( \"precision\" , performance [ \"overall\" ][ \"precision\" ]) trial . set_user_attr ( \"recall\" , performance [ \"overall\" ][ \"recall\" ]) trial . set_user_attr ( \"f1\" , performance [ \"overall\" ][ \"f1\" ]) return performance [ \"overall\" ][ \"f1\" ] train ( params , trial = None ) Operations for training. Parameters: Name Type Description Default params Namespace Input parameters for operations. required trial Trial Optuna optimization trial. Defaults to None. None Returns: Type Description Dict Artifacts to save and load for later. Source code in tagifai/train.py def train ( params : Namespace , trial : optuna . trial . _trial . Trial = None ) -> Dict : \"\"\"Operations for training. Args: params (Namespace): Input parameters for operations. trial (optuna.trial._trial.Trail, optional): Optuna optimization trial. Defaults to None. Returns: Artifacts to save and load for later. \"\"\" # Set up utils . set_seed ( seed = params . seed ) device = utils . set_device ( cuda = params . cuda ) # Load features features_fp = Path ( config . DATA_DIR , \"features.json\" ) tags_fp = Path ( config . DATA_DIR , \"tags.json\" ) features = utils . load_dict ( filepath = features_fp ) tags_dict = utils . list_to_dict ( utils . load_dict ( filepath = tags_fp ), key = \"tag\" ) df = pd . DataFrame ( features ) if params . shuffle : df = df . sample ( frac = 1 ) . reset_index ( drop = True ) df = df [: params . subset ] # None = all samples # Prepare data (filter, clean, etc.) df , tags_above_freq , tags_below_freq = data . prepare ( df = df , include = list ( tags_dict . keys ()), exclude = config . EXCLUDED_TAGS , min_tag_freq = params . min_tag_freq , ) params . num_samples = len ( df ) # Preprocess data df . text = df . text . apply ( data . preprocess , lower = params . lower , stem = params . stem ) # Encode labels labels = df . tags label_encoder = data . MultiLabelLabelEncoder () label_encoder . fit ( labels ) y = label_encoder . encode ( labels ) # Class weights all_tags = list ( itertools . chain . from_iterable ( labels . values )) counts = np . bincount ([ label_encoder . class_to_index [ class_ ] for class_ in all_tags ]) class_weights = { i : 1.0 / count for i , count in enumerate ( counts )} # Split data utils . set_seed ( seed = params . seed ) # needed for skmultilearn X = df . text . to_numpy () X_train , X_ , y_train , y_ = data . iterative_train_test_split ( X = X , y = y , train_size = params . train_size ) X_val , X_test , y_val , y_test = data . iterative_train_test_split ( X = X_ , y = y_ , train_size = 0.5 ) test_df = pd . DataFrame ({ \"text\" : X_test , \"tags\" : label_encoder . decode ( y_test )}) # Tokenize inputs tokenizer = data . Tokenizer ( char_level = params . char_level ) tokenizer . fit_on_texts ( texts = X_train ) X_train = np . array ( tokenizer . texts_to_sequences ( X_train ), dtype = object ) X_val = np . array ( tokenizer . texts_to_sequences ( X_val ), dtype = object ) X_test = np . array ( tokenizer . texts_to_sequences ( X_test ), dtype = object ) # Create dataloaders train_dataset = data . CNNTextDataset ( X = X_train , y = y_train , max_filter_size = params . max_filter_size ) val_dataset = data . CNNTextDataset ( X = X_val , y = y_val , max_filter_size = params . max_filter_size ) train_dataloader = train_dataset . create_dataloader ( batch_size = params . batch_size ) val_dataloader = val_dataset . create_dataloader ( batch_size = params . batch_size ) # Initialize model model = models . initialize_model ( params = params , vocab_size = len ( tokenizer ), num_classes = len ( label_encoder ), device = device , ) # Train model logger . info ( f \"Parameters: { json . dumps ( params . __dict__ , indent = 2 , cls = NumpyEncoder ) } \" ) class_weights_tensor = torch . Tensor ( np . array ( list ( class_weights . values ()))) loss_fn = nn . BCEWithLogitsLoss ( weight = class_weights_tensor ) optimizer = torch . optim . Adam ( model . parameters (), lr = params . lr ) scheduler = torch . optim . lr_scheduler . ReduceLROnPlateau ( optimizer , mode = \"min\" , factor = 0.05 , patience = 5 ) # Trainer module trainer = Trainer ( model = model , device = device , loss_fn = loss_fn , optimizer = optimizer , scheduler = scheduler , trial = trial , ) # Train best_val_loss , best_model = trainer . train ( params . num_epochs , params . patience , train_dataloader , val_dataloader ) # Find best threshold _ , y_true , y_prob = trainer . eval_step ( dataloader = train_dataloader ) params . threshold = find_best_threshold ( y_true = y_true , y_prob = y_prob ) # Evaluate model artifacts = { \"params\" : params , \"label_encoder\" : label_encoder , \"tokenizer\" : tokenizer , \"model\" : best_model , \"loss\" : best_val_loss , } device = torch . device ( \"cpu\" ) y_true , y_pred , performance = eval . evaluate ( df = test_df , artifacts = artifacts ) artifacts [ \"performance\" ] = performance return artifacts","title":"Training"},{"location":"tagifai/train/#tagifai.train.Trainer","text":"Object used to facilitate training.","title":"Trainer"},{"location":"tagifai/train/#tagifai.train.Trainer.eval_step","text":"Evaluation (val / test) step. Parameters: Name Type Description Default dataloader DataLoader Torch dataloader to load batches from. required Source code in tagifai/train.py def eval_step ( self , dataloader : torch . utils . data . DataLoader ): \"\"\"Evaluation (val / test) step. Args: dataloader (torch.utils.data.DataLoader): Torch dataloader to load batches from. \"\"\" # Set model to eval mode self . model . eval () loss = 0.0 y_trues , y_probs = [], [] # Iterate over val batches with torch . inference_mode (): for i , batch in enumerate ( dataloader ): # Step batch = [ item . to ( self . device ) for item in batch ] # Set device inputs , y_true = batch [: - 1 ], batch [ - 1 ] z = self . model ( inputs ) # Forward pass J = self . loss_fn ( z , y_true ) . item () # Cumulative Metrics loss += ( J - loss ) / ( i + 1 ) # Store outputs y_prob = torch . sigmoid ( z ) . cpu () . numpy () y_probs . extend ( y_prob ) y_trues . extend ( y_true . cpu () . numpy ()) return loss , np . vstack ( y_trues ), np . vstack ( y_probs )","title":"eval_step()"},{"location":"tagifai/train/#tagifai.train.Trainer.predict_step","text":"Prediction (inference) step. Note Loss is not calculated for this loop. Parameters: Name Type Description Default dataloader DataLoader Torch dataloader to load batches from. required Source code in tagifai/train.py def predict_step ( self , dataloader : torch . utils . data . DataLoader ): \"\"\"Prediction (inference) step. Note: Loss is not calculated for this loop. Args: dataloader (torch.utils.data.DataLoader): Torch dataloader to load batches from. \"\"\" # Set model to eval mode self . model . eval () y_trues , y_probs = [], [] # Iterate over batches with torch . inference_mode (): for i , batch in enumerate ( dataloader ): # Forward pass w/ inputs batch = [ item . to ( self . device ) for item in batch ] # Set device inputs , y_true = batch [: - 1 ], batch [ - 1 ] z = self . model ( inputs ) # Store outputs y_prob = torch . sigmoid ( z ) . cpu () . numpy () y_probs . extend ( y_prob ) y_trues . extend ( y_true . cpu () . numpy ()) return np . vstack ( y_trues ), np . vstack ( y_probs )","title":"predict_step()"},{"location":"tagifai/train/#tagifai.train.Trainer.train","text":"Training loop. Parameters: Name Type Description Default num_epochs int Maximum number of epochs to train for (can stop earlier based on performance). required patience int Number of acceptable epochs for continuous degrading performance. required train_dataloader DataLoader Dataloader object with training data split. required val_dataloader DataLoader Dataloader object with validation data split. required Exceptions: Type Description optuna.TrialPruned Early stopping of the optimization trial if poor performance. Returns: Type Description Tuple The best validation loss and the trained model from that point. Source code in tagifai/train.py def train ( self , num_epochs : int , patience : int , train_dataloader : torch . utils . data . DataLoader , val_dataloader : torch . utils . data . DataLoader , ) -> Tuple : \"\"\"Training loop. Args: num_epochs (int): Maximum number of epochs to train for (can stop earlier based on performance). patience (int): Number of acceptable epochs for continuous degrading performance. train_dataloader (torch.utils.data.DataLoader): Dataloader object with training data split. val_dataloader (torch.utils.data.DataLoader): Dataloader object with validation data split. Raises: optuna.TrialPruned: Early stopping of the optimization trial if poor performance. Returns: The best validation loss and the trained model from that point. \"\"\" best_val_loss = np . inf best_model = None _patience = patience for epoch in range ( num_epochs ): # Steps train_loss = self . train_step ( dataloader = train_dataloader ) val_loss , _ , _ = self . eval_step ( dataloader = val_dataloader ) self . scheduler . step ( val_loss ) # Pruning based on the intermediate value if self . trial : self . trial . report ( val_loss , epoch ) if self . trial . should_prune (): # pragma: no cover, optuna pruning logger . info ( \"Unpromising trial pruned!\" ) raise optuna . TrialPruned () # Early stopping if val_loss < best_val_loss : best_val_loss = val_loss best_model = self . model _patience = patience # reset _patience else : # pragma: no cover, simple subtraction _patience -= 1 if not _patience : # pragma: no cover, simple break logger . info ( \"Stopping early!\" ) break # Logging logger . info ( f \"Epoch: { epoch + 1 } | \" f \"train_loss: { train_loss : .5f } , \" f \"val_loss: { val_loss : .5f } , \" f \"lr: { self . optimizer . param_groups [ 0 ][ 'lr' ] : .2E } , \" f \"_patience: { _patience } \" ) return best_val_loss , best_model","title":"train()"},{"location":"tagifai/train/#tagifai.train.Trainer.train_step","text":"Train step. Parameters: Name Type Description Default dataloader DataLoader Torch dataloader to load batches from. required Source code in tagifai/train.py def train_step ( self , dataloader : torch . utils . data . DataLoader ): \"\"\"Train step. Args: dataloader (torch.utils.data.DataLoader): Torch dataloader to load batches from. \"\"\" # Set model to train mode self . model . train () loss = 0.0 # Iterate over train batches for i , batch in enumerate ( dataloader ): # Step batch = [ item . to ( self . device ) for item in batch ] # Set device inputs , targets = batch [: - 1 ], batch [ - 1 ] self . optimizer . zero_grad () # Reset gradients z = self . model ( inputs ) # Forward pass J = self . loss_fn ( z , targets ) # Define loss J . backward () # Backward pass self . optimizer . step () # Update weights # Cumulative Metrics loss += ( J . detach () . item () - loss ) / ( i + 1 ) return loss","title":"train_step()"},{"location":"tagifai/train/#tagifai.train.find_best_threshold","text":"Determine the best threshold for maximum f1 score. Usage: # Find best threshold _ , y_true , y_prob = trainer . eval_step ( dataloader = train_dataloader ) params . threshold = find_best_threshold ( y_true = y_true , y_prob = y_prob ) Parameters: Name Type Description Default y_true ndarray True labels. required y_prob ndarray Probability distribution for predicted labels. required Returns: Type Description float Best threshold for maximum f1 score. Source code in tagifai/train.py def find_best_threshold ( y_true : np . ndarray , y_prob : np . ndarray ) -> float : \"\"\"Determine the best threshold for maximum f1 score. Usage: ```python # Find best threshold _, y_true, y_prob = trainer.eval_step(dataloader=train_dataloader) params.threshold = find_best_threshold(y_true=y_true, y_prob=y_prob) ``` Args: y_true (np.ndarray): True labels. y_prob (np.ndarray): Probability distribution for predicted labels. Returns: Best threshold for maximum f1 score. \"\"\" precisions , recalls , thresholds = precision_recall_curve ( y_true . ravel (), y_prob . ravel ()) f1s = ( 2 * precisions * recalls ) / ( precisions + recalls ) return thresholds [ np . argmax ( f1s )]","title":"find_best_threshold()"},{"location":"tagifai/train/#tagifai.train.objective","text":"Objective function for optimization trials. Parameters: Name Type Description Default params Namespace Input parameters for each trial (see config/params.json ). required trial Trial Optuna optimization trial. required Returns: Type Description float F1 score from evaluating the trained model on the test data split. Source code in tagifai/train.py def objective ( params : Namespace , trial : optuna . trial . _trial . Trial ) -> float : \"\"\"Objective function for optimization trials. Args: params (Namespace): Input parameters for each trial (see `config/params.json`). trial (optuna.trial._trial.Trial): Optuna optimization trial. Returns: F1 score from evaluating the trained model on the test data split. \"\"\" # Paramters (to tune) params . embedding_dim = trial . suggest_int ( \"embedding_dim\" , 128 , 512 ) params . num_filters = trial . suggest_int ( \"num_filters\" , 128 , 512 ) params . hidden_dim = trial . suggest_int ( \"hidden_dim\" , 128 , 512 ) params . dropout_p = trial . suggest_uniform ( \"dropout_p\" , 0.3 , 0.8 ) params . lr = trial . suggest_loguniform ( \"lr\" , 5e-5 , 5e-4 ) # Train (can move some of these outside for efficiency) logger . info ( f \" \\n Trial { trial . number } :\" ) logger . info ( json . dumps ( trial . params , indent = 2 )) artifacts = train ( params = params , trial = trial ) # Set additional attributes params = artifacts [ \"params\" ] performance = artifacts [ \"performance\" ] logger . info ( json . dumps ( performance [ \"overall\" ], indent = 2 )) trial . set_user_attr ( \"threshold\" , params . threshold ) trial . set_user_attr ( \"precision\" , performance [ \"overall\" ][ \"precision\" ]) trial . set_user_attr ( \"recall\" , performance [ \"overall\" ][ \"recall\" ]) trial . set_user_attr ( \"f1\" , performance [ \"overall\" ][ \"f1\" ]) return performance [ \"overall\" ][ \"f1\" ]","title":"objective()"},{"location":"tagifai/train/#tagifai.train.train","text":"Operations for training. Parameters: Name Type Description Default params Namespace Input parameters for operations. required trial Trial Optuna optimization trial. Defaults to None. None Returns: Type Description Dict Artifacts to save and load for later. Source code in tagifai/train.py def train ( params : Namespace , trial : optuna . trial . _trial . Trial = None ) -> Dict : \"\"\"Operations for training. Args: params (Namespace): Input parameters for operations. trial (optuna.trial._trial.Trail, optional): Optuna optimization trial. Defaults to None. Returns: Artifacts to save and load for later. \"\"\" # Set up utils . set_seed ( seed = params . seed ) device = utils . set_device ( cuda = params . cuda ) # Load features features_fp = Path ( config . DATA_DIR , \"features.json\" ) tags_fp = Path ( config . DATA_DIR , \"tags.json\" ) features = utils . load_dict ( filepath = features_fp ) tags_dict = utils . list_to_dict ( utils . load_dict ( filepath = tags_fp ), key = \"tag\" ) df = pd . DataFrame ( features ) if params . shuffle : df = df . sample ( frac = 1 ) . reset_index ( drop = True ) df = df [: params . subset ] # None = all samples # Prepare data (filter, clean, etc.) df , tags_above_freq , tags_below_freq = data . prepare ( df = df , include = list ( tags_dict . keys ()), exclude = config . EXCLUDED_TAGS , min_tag_freq = params . min_tag_freq , ) params . num_samples = len ( df ) # Preprocess data df . text = df . text . apply ( data . preprocess , lower = params . lower , stem = params . stem ) # Encode labels labels = df . tags label_encoder = data . MultiLabelLabelEncoder () label_encoder . fit ( labels ) y = label_encoder . encode ( labels ) # Class weights all_tags = list ( itertools . chain . from_iterable ( labels . values )) counts = np . bincount ([ label_encoder . class_to_index [ class_ ] for class_ in all_tags ]) class_weights = { i : 1.0 / count for i , count in enumerate ( counts )} # Split data utils . set_seed ( seed = params . seed ) # needed for skmultilearn X = df . text . to_numpy () X_train , X_ , y_train , y_ = data . iterative_train_test_split ( X = X , y = y , train_size = params . train_size ) X_val , X_test , y_val , y_test = data . iterative_train_test_split ( X = X_ , y = y_ , train_size = 0.5 ) test_df = pd . DataFrame ({ \"text\" : X_test , \"tags\" : label_encoder . decode ( y_test )}) # Tokenize inputs tokenizer = data . Tokenizer ( char_level = params . char_level ) tokenizer . fit_on_texts ( texts = X_train ) X_train = np . array ( tokenizer . texts_to_sequences ( X_train ), dtype = object ) X_val = np . array ( tokenizer . texts_to_sequences ( X_val ), dtype = object ) X_test = np . array ( tokenizer . texts_to_sequences ( X_test ), dtype = object ) # Create dataloaders train_dataset = data . CNNTextDataset ( X = X_train , y = y_train , max_filter_size = params . max_filter_size ) val_dataset = data . CNNTextDataset ( X = X_val , y = y_val , max_filter_size = params . max_filter_size ) train_dataloader = train_dataset . create_dataloader ( batch_size = params . batch_size ) val_dataloader = val_dataset . create_dataloader ( batch_size = params . batch_size ) # Initialize model model = models . initialize_model ( params = params , vocab_size = len ( tokenizer ), num_classes = len ( label_encoder ), device = device , ) # Train model logger . info ( f \"Parameters: { json . dumps ( params . __dict__ , indent = 2 , cls = NumpyEncoder ) } \" ) class_weights_tensor = torch . Tensor ( np . array ( list ( class_weights . values ()))) loss_fn = nn . BCEWithLogitsLoss ( weight = class_weights_tensor ) optimizer = torch . optim . Adam ( model . parameters (), lr = params . lr ) scheduler = torch . optim . lr_scheduler . ReduceLROnPlateau ( optimizer , mode = \"min\" , factor = 0.05 , patience = 5 ) # Trainer module trainer = Trainer ( model = model , device = device , loss_fn = loss_fn , optimizer = optimizer , scheduler = scheduler , trial = trial , ) # Train best_val_loss , best_model = trainer . train ( params . num_epochs , params . patience , train_dataloader , val_dataloader ) # Find best threshold _ , y_true , y_prob = trainer . eval_step ( dataloader = train_dataloader ) params . threshold = find_best_threshold ( y_true = y_true , y_prob = y_prob ) # Evaluate model artifacts = { \"params\" : params , \"label_encoder\" : label_encoder , \"tokenizer\" : tokenizer , \"model\" : best_model , \"loss\" : best_val_loss , } device = torch . device ( \"cpu\" ) y_true , y_pred , performance = eval . evaluate ( df = test_df , artifacts = artifacts ) artifacts [ \"performance\" ] = performance return artifacts","title":"train()"},{"location":"tagifai/utils/","text":"list_to_dict ( list_of_dicts , key ) Convert a list of dict_a to a dict_b where the key in dict_b is an item in each dict_a . Parameters: Name Type Description Default list_of_dicts List list of items to convert to dict. required key str Name of the item in dict_a to use as primary key for dict_b . required Returns: Type Description Dict A dictionary with items from the list organized by key. Source code in tagifai/utils.py def list_to_dict ( list_of_dicts : List , key : str ) -> Dict : \"\"\"Convert a list of `dict_a` to a `dict_b` where the `key` in `dict_b` is an item in each `dict_a`. Args: list_of_dicts (List): list of items to convert to dict. key (str): Name of the item in `dict_a` to use as primary key for `dict_b`. Returns: A dictionary with items from the list organized by key. \"\"\" d_b = {} for d_a in list_of_dicts : d_b_key = d_a . pop ( key ) d_b [ d_b_key ] = d_a return d_b load_dict ( filepath ) Load a dictionary from a JSON's filepath. Parameters: Name Type Description Default filepath str JSON's filepath. required Returns: Type Description Dict A dictionary with the data loaded. Source code in tagifai/utils.py def load_dict ( filepath : str ) -> Dict : \"\"\"Load a dictionary from a JSON's filepath. Args: filepath (str): JSON's filepath. Returns: A dictionary with the data loaded. \"\"\" with open ( filepath ) as fp : d = json . load ( fp ) return d load_json_from_url ( url ) Load JSON data from a URL. Parameters: Name Type Description Default url str URL of the data source. required Returns: Type Description Dict A dictionary with the loaded JSON data. Source code in tagifai/utils.py def load_json_from_url ( url : str ) -> Dict : \"\"\"Load JSON data from a URL. Args: url (str): URL of the data source. Returns: A dictionary with the loaded JSON data. \"\"\" data = json . loads ( urlopen ( url ) . read ()) return data save_dict ( d , filepath , cls = None , sortkeys = False ) Save a dictionary to a specific location. Warning This will overwrite any existing file at filepath . Parameters: Name Type Description Default d Dict dictionary to save. required filepath str location to save the dictionary to as a JSON file. required cls optional encoder to use on dict data. Defaults to None. None sortkeys bool sort keys in dict alphabetically. Defaults to False. False Source code in tagifai/utils.py def save_dict ( d : Dict , filepath : str , cls = None , sortkeys : bool = False ) -> None : \"\"\"Save a dictionary to a specific location. Warning: This will overwrite any existing file at `filepath`. Args: d (Dict): dictionary to save. filepath (str): location to save the dictionary to as a JSON file. cls (optional): encoder to use on dict data. Defaults to None. sortkeys (bool, optional): sort keys in dict alphabetically. Defaults to False. \"\"\" with open ( filepath , \"w\" ) as fp : json . dump ( d , indent = 2 , fp = fp , cls = cls , sort_keys = sortkeys ) set_device ( cuda ) Set the device for computation. Parameters: Name Type Description Default cuda bool Determine whether to use GPU or not (if available). required Returns: Type Description device Device that will be use for compute. Source code in tagifai/utils.py def set_device ( cuda : bool ) -> torch . device : \"\"\"Set the device for computation. Args: cuda (bool): Determine whether to use GPU or not (if available). Returns: Device that will be use for compute. \"\"\" device = torch . device ( \"cuda\" if ( torch . cuda . is_available () and cuda ) else \"cpu\" ) torch . set_default_tensor_type ( \"torch.FloatTensor\" ) if device . type == \"cuda\" : # pragma: no cover, simple tensor type setting torch . set_default_tensor_type ( \"torch.cuda.FloatTensor\" ) return device set_seed ( seed = 1234 ) Set seed for reproducibility. Parameters: Name Type Description Default seed int number to use as the seed. Defaults to 1234. 1234 Source code in tagifai/utils.py def set_seed ( seed : int = 1234 ) -> None : \"\"\"Set seed for reproducibility. Args: seed (int, optional): number to use as the seed. Defaults to 1234. \"\"\" # Set seeds np . random . seed ( seed ) random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) # multi-GPU","title":"Utilities"},{"location":"tagifai/utils/#tagifai.utils.list_to_dict","text":"Convert a list of dict_a to a dict_b where the key in dict_b is an item in each dict_a . Parameters: Name Type Description Default list_of_dicts List list of items to convert to dict. required key str Name of the item in dict_a to use as primary key for dict_b . required Returns: Type Description Dict A dictionary with items from the list organized by key. Source code in tagifai/utils.py def list_to_dict ( list_of_dicts : List , key : str ) -> Dict : \"\"\"Convert a list of `dict_a` to a `dict_b` where the `key` in `dict_b` is an item in each `dict_a`. Args: list_of_dicts (List): list of items to convert to dict. key (str): Name of the item in `dict_a` to use as primary key for `dict_b`. Returns: A dictionary with items from the list organized by key. \"\"\" d_b = {} for d_a in list_of_dicts : d_b_key = d_a . pop ( key ) d_b [ d_b_key ] = d_a return d_b","title":"list_to_dict()"},{"location":"tagifai/utils/#tagifai.utils.load_dict","text":"Load a dictionary from a JSON's filepath. Parameters: Name Type Description Default filepath str JSON's filepath. required Returns: Type Description Dict A dictionary with the data loaded. Source code in tagifai/utils.py def load_dict ( filepath : str ) -> Dict : \"\"\"Load a dictionary from a JSON's filepath. Args: filepath (str): JSON's filepath. Returns: A dictionary with the data loaded. \"\"\" with open ( filepath ) as fp : d = json . load ( fp ) return d","title":"load_dict()"},{"location":"tagifai/utils/#tagifai.utils.load_json_from_url","text":"Load JSON data from a URL. Parameters: Name Type Description Default url str URL of the data source. required Returns: Type Description Dict A dictionary with the loaded JSON data. Source code in tagifai/utils.py def load_json_from_url ( url : str ) -> Dict : \"\"\"Load JSON data from a URL. Args: url (str): URL of the data source. Returns: A dictionary with the loaded JSON data. \"\"\" data = json . loads ( urlopen ( url ) . read ()) return data","title":"load_json_from_url()"},{"location":"tagifai/utils/#tagifai.utils.save_dict","text":"Save a dictionary to a specific location. Warning This will overwrite any existing file at filepath . Parameters: Name Type Description Default d Dict dictionary to save. required filepath str location to save the dictionary to as a JSON file. required cls optional encoder to use on dict data. Defaults to None. None sortkeys bool sort keys in dict alphabetically. Defaults to False. False Source code in tagifai/utils.py def save_dict ( d : Dict , filepath : str , cls = None , sortkeys : bool = False ) -> None : \"\"\"Save a dictionary to a specific location. Warning: This will overwrite any existing file at `filepath`. Args: d (Dict): dictionary to save. filepath (str): location to save the dictionary to as a JSON file. cls (optional): encoder to use on dict data. Defaults to None. sortkeys (bool, optional): sort keys in dict alphabetically. Defaults to False. \"\"\" with open ( filepath , \"w\" ) as fp : json . dump ( d , indent = 2 , fp = fp , cls = cls , sort_keys = sortkeys )","title":"save_dict()"},{"location":"tagifai/utils/#tagifai.utils.set_device","text":"Set the device for computation. Parameters: Name Type Description Default cuda bool Determine whether to use GPU or not (if available). required Returns: Type Description device Device that will be use for compute. Source code in tagifai/utils.py def set_device ( cuda : bool ) -> torch . device : \"\"\"Set the device for computation. Args: cuda (bool): Determine whether to use GPU or not (if available). Returns: Device that will be use for compute. \"\"\" device = torch . device ( \"cuda\" if ( torch . cuda . is_available () and cuda ) else \"cpu\" ) torch . set_default_tensor_type ( \"torch.FloatTensor\" ) if device . type == \"cuda\" : # pragma: no cover, simple tensor type setting torch . set_default_tensor_type ( \"torch.cuda.FloatTensor\" ) return device","title":"set_device()"},{"location":"tagifai/utils/#tagifai.utils.set_seed","text":"Set seed for reproducibility. Parameters: Name Type Description Default seed int number to use as the seed. Defaults to 1234. 1234 Source code in tagifai/utils.py def set_seed ( seed : int = 1234 ) -> None : \"\"\"Set seed for reproducibility. Args: seed (int, optional): number to use as the seed. Defaults to 1234. \"\"\" # Set seeds np . random . seed ( seed ) random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) # multi-GPU","title":"set_seed()"}]}